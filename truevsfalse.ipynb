{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Fact-checking project using pandas and ScikitLearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travail réalisé par : Bouali Mohammed-Amin, Oussama Nassim Sehout, Chahinez Benallal, Abdellah Choukri \n",
    "\n",
    "\n",
    "\n",
    "###  Sommaire du travail :\n",
    " - Préparation de l'environnement du travail\n",
    " - Chargement et analyse du jeu de données\n",
    " - Prétraitement du texte et traitement des valeurs manquantes\n",
    " - Conversion des textes aux valeurs numériques\n",
    " - Division du jeu de données en données d'entraînement et données de test\n",
    " - Entraînement et prédiction en utilisant des modèles de classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape 0: Préparation de l'environnement du travail \n",
    "\n",
    "- pip install inflect\n",
    "- pip install contractions (on a commenté cette partie du prétraitement donc ce n'est plus nécessaire)\n",
    "- pip install autocorrect  (on a commenté cette partie du prétraitement donc ce n'est plus nécessaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape 1 : Chargement et analyse du jeu de données  \n",
    "  On a généré deux fichiers csv, un avec valeurs true, et l'autre avec valeurs false à partir du site ClaimsKg, après on les concatène pour avoir notre jeu de données, ensuite on affiche les informations nécessaires qui peuvent nous aider à bien comprendre de quoi notre jeu de données est constitué."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataframe :202328 (14452*14)\n",
      "\n",
      "--------- Afficher le nombre d'entrées NaN pour chaque colonne de notre dataframe ---------\n",
      "\n",
      "id                           0\n",
      "text                         0\n",
      "date                         0\n",
      "truthRating                  0\n",
      "ratingName                   0\n",
      "author                       0\n",
      "headline                     0\n",
      "named_entities_claim        27\n",
      "named_entities_article    4865\n",
      "keywords                   973\n",
      "source                       0\n",
      "sourceURL                    0\n",
      "link                         0\n",
      "language                     0\n",
      "dtype: int64\n",
      "\n",
      "--------- Afficher les différentes informations de notre dataset ---------\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 14452 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      "id                        14452 non-null object\n",
      "text                      14452 non-null object\n",
      "date                      14452 non-null object\n",
      "truthRating               14452 non-null int64\n",
      "ratingName                14452 non-null bool\n",
      "author                    14452 non-null object\n",
      "headline                  14452 non-null object\n",
      "named_entities_claim      14425 non-null object\n",
      "named_entities_article    9587 non-null object\n",
      "keywords                  13479 non-null object\n",
      "source                    14452 non-null object\n",
      "sourceURL                 14452 non-null object\n",
      "link                      14452 non-null object\n",
      "language                  14452 non-null object\n",
      "dtypes: bool(1), int64(1), object(12)\n",
      "memory usage: 1.6+ MB\n",
      "None\n",
      "\n",
      "--------- Afficher les 20 premières entrées de notre dataset ---------\n",
      "\n",
      "                                                   id  \\\n",
      "0   http://data.gesis.org/claimskg/claim_review/36...   \n",
      "1   http://data.gesis.org/claimskg/claim_review/e6...   \n",
      "2   http://data.gesis.org/claimskg/claim_review/e0...   \n",
      "3   http://data.gesis.org/claimskg/claim_review/48...   \n",
      "4   http://data.gesis.org/claimskg/claim_review/80...   \n",
      "5   http://data.gesis.org/claimskg/claim_review/2c...   \n",
      "6   http://data.gesis.org/claimskg/claim_review/3a...   \n",
      "7   http://data.gesis.org/claimskg/claim_review/90...   \n",
      "8   http://data.gesis.org/claimskg/claim_review/09...   \n",
      "9   http://data.gesis.org/claimskg/claim_review/7a...   \n",
      "10  http://data.gesis.org/claimskg/claim_review/58...   \n",
      "11  http://data.gesis.org/claimskg/claim_review/c4...   \n",
      "12  http://data.gesis.org/claimskg/claim_review/00...   \n",
      "13  http://data.gesis.org/claimskg/claim_review/97...   \n",
      "14  http://data.gesis.org/claimskg/claim_review/66...   \n",
      "15  http://data.gesis.org/claimskg/claim_review/86...   \n",
      "16  http://data.gesis.org/claimskg/claim_review/0f...   \n",
      "17  http://data.gesis.org/claimskg/claim_review/4c...   \n",
      "18  http://data.gesis.org/claimskg/claim_review/a4...   \n",
      "19  http://data.gesis.org/claimskg/claim_review/28...   \n",
      "\n",
      "                                                 text        date  \\\n",
      "0   'There will be no public funding for abortion ...  2010-03-21   \n",
      "1   Central Health 'is the only hospital district ...  2011-03-15   \n",
      "2   Says most of Perry's chiefs of staff have been...  2010-08-14   \n",
      "3   Says 'as Co-Chair of the Joint Ways & Means Co...  2012-09-28   \n",
      "4   Says Gary Farmer’s claim that he 'received an ...  2016-07-08   \n",
      "5   Democrats say Chafee wants to tax equipment th...  2010-07-23   \n",
      "6   Will Hurd 'tends to have a 96 percent straight...  2015-10-23   \n",
      "7   Roger Stone once marketed 'Hillary for Prison'...     Unknown   \n",
      "8   A video shows a group of Muslims ripping up an...     Unknown   \n",
      "9   A U.S. serviceman wrote a poem describing a so...     Unknown   \n",
      "10  Says Republicans supported legislation on earl...  2012-08-15   \n",
      "11  ”Since its inception 3.881 million tonnes of c...     Unknown   \n",
      "12  During the Jim Crow era of racial segregation ...     Unknown   \n",
      "13  'Our businesses have created jobs every single...  2016-01-12   \n",
      "14  A survivor of the September 11 terrorist attac...     Unknown   \n",
      "15  Condoleezza Rice shopped for expensive shoes d...     Unknown   \n",
      "16  The Buffett Rule 'will bring in less than $5 b...  2012-04-16   \n",
      "17  A spill by the Environmental Protection Agency...     Unknown   \n",
      "18  Says he 'is the only candidate in the race for...  2012-05-04   \n",
      "19  Missouri Republicans passed a bill to lower th...     Unknown   \n",
      "\n",
      "    truthRating  ratingName                            author  \\\n",
      "0             3        True                       Bart Stupak   \n",
      "1             3        True                   Wayne Christian   \n",
      "2             3        True                        Bill White   \n",
      "3             3        True                        Mary Nolan   \n",
      "4             3        True                       Jim Waldman   \n",
      "5             3        True  Democratic Governors Association   \n",
      "6             3        True                      Pete Gallego   \n",
      "7             3        True                           Unknown   \n",
      "8             3        True                           Unknown   \n",
      "9             3        True                           Unknown   \n",
      "10            3        True                      Teresa Fedor   \n",
      "11            3        True                           Unknown   \n",
      "12            3        True                           Unknown   \n",
      "13            3        True                      Barack Obama   \n",
      "14            3        True                           Unknown   \n",
      "15            3        True                           Unknown   \n",
      "16            3        True                       Rob Portman   \n",
      "17            3        True                           Unknown   \n",
      "18            3        True                       John Ludlow   \n",
      "19            3        True                           Unknown   \n",
      "\n",
      "                                             headline  \\\n",
      "0   Stupak revises abortion stance on health care ...   \n",
      "1   State Rep. Wayne Christian says Central Health...   \n",
      "2   Bill White says most of Gov. Rick Perry's chie...   \n",
      "3   Did Mary Nolan secure funding for Milwaukie br...   \n",
      "4   Florida Senate candidate never actually receiv...   \n",
      "5     Democrats say Chafee would tax amputee veterans   \n",
      "6   Pete Gallego says Will Hurd votes with House R...   \n",
      "7   Did Roger Stone Sell ‘Hillary for Prison’ Merc...   \n",
      "8   Did This Video Show Muslims Ripping Up an Amer...   \n",
      "9                The Soldier’s Night Before Christmas   \n",
      "10  Teresa Fedor says Republicans supported Ohio's...   \n",
      "11  State of the Nation 2019: Did Kenyatta get his...   \n",
      "12  Did ‘No Whites Allowed’ Signs Exist in the Seg...   \n",
      "13  Business has created jobs every month since Ob...   \n",
      "14                           WTC Survivor Ferry Death   \n",
      "15                   Condoleezza Rice Shops for Shoes   \n",
      "16  Rob Portman says Buffett Rule would raise just...   \n",
      "17          Did the EPA Cause the Animas River Spill?   \n",
      "18  Is John Ludlow the only candidate in the race ...   \n",
      "19  Did Missouri Republicans Pass a Law Lowering t...   \n",
      "\n",
      "                                 named_entities_claim  \\\n",
      "0   Abortion rights,Barack Obama,Bart Stupak,Ben N...   \n",
      "1   Austin American-Statesman,Harris County Hospit...   \n",
      "2   AT&T,Bill Clements,Bill White,Bracewell & Giul...   \n",
      "3   Carolyn Tomei,Dave Hunt,Fetsch,Jeff Merkley,Ka...   \n",
      "4   Gary Farmer,Gwyndolen Clarke-Reed,Jim Waldman,...   \n",
      "5   Democratic Governors Association,John Chafee,K...   \n",
      "6   114th Congress,2016 general election,CQ Roll C...   \n",
      "7   2016 presidential campaign,Donald Trump,Hillar...   \n",
      "8   Cuba,Facebook,Guantanamo Bay,Guantánamo Bay,Gu...   \n",
      "9   Afghanistan,Christmas decoration,Japan,Lance C...   \n",
      "10  2008 election,Barack Obama,Bob Taft,Democrat,J...   \n",
      "11  2.2 million,Africa Check,Garissa,Institute of ...   \n",
      "12  African American,Augusta Chronicle,Augusta, Ge...   \n",
      "13  Affordable Care Act,Barack Obama,Bureau of Lab...   \n",
      "14  11 September 2001 terrorist attacks,American A...   \n",
      "15  Condoleezza Rice,Dr. Condoleezza Rice,Hurrican...   \n",
      "16  Buffet rule,Buffett Rule,Bush tax cuts,Congres...   \n",
      "17  Animas River,EPA,Environmental Protection Agen...   \n",
      "18  Clackamas County,Dave Hunt,Oregon Revised Stat...   \n",
      "19  Eric Greitens,Federal Reserve Bank of St. Loui...   \n",
      "\n",
      "                               named_entities_article  \\\n",
      "0                                            abortion   \n",
      "1                                                 NaN   \n",
      "2                                                 NaN   \n",
      "3               Portland-Milwaukie Light Rail project   \n",
      "4                                         Gary Farmer   \n",
      "5                                                 NaN   \n",
      "6                                Republican,Will Hurd   \n",
      "7                      Hillary for Prison,Roger Stone   \n",
      "8                                       New York City   \n",
      "9                              night before Christmas   \n",
      "10                                       early voting   \n",
      "11                                        2.2 million   \n",
      "12  American South,Jim Crow,Jim Crow era,racial se...   \n",
      "13                                                NaN   \n",
      "14  September 11 terrorist attacks,World Trade Center   \n",
      "15                 Condoleezza Rice,Hurricane Katrina   \n",
      "16                                       Buffett Rule   \n",
      "17                                       Animas River   \n",
      "18                                   Clackamas County   \n",
      "19                              Missouri,minimum wage   \n",
      "\n",
      "                                             keywords       source  \\\n",
      "0                                Abortion,Health Care   politifact   \n",
      "1                                            Abortion   politifact   \n",
      "2                                              Ethics   politifact   \n",
      "3          State Budget,State Finances,Transportation   politifact   \n",
      "4                                                Guns   politifact   \n",
      "5                                               Taxes   politifact   \n",
      "6   Bipartisanship,Congress,Redistricting,Voting R...   politifact   \n",
      "7                                                 NaN       snopes   \n",
      "8                             flag desecration, islam       snopes   \n",
      "9                                           christmas       snopes   \n",
      "10      Elections,Government regulation,Voting Record   politifact   \n",
      "11  development, economy, State of the Nation, Uhu...  africacheck   \n",
      "12                                                NaN       snopes   \n",
      "13                           Economy,Health Care,Jobs   politifact   \n",
      "14                                        ASP Article       snopes   \n",
      "15                                        ASP Article       snopes   \n",
      "16                       Deficit,Federal Budget,Taxes   politifact   \n",
      "17  animas river, environmental protection agency,...       snopes   \n",
      "18  County Budget,County Government,Message Machin...   politifact   \n",
      "19                             minimum wage, missouri       snopes   \n",
      "\n",
      "                    sourceURL  \\\n",
      "0   http://www.politifact.com   \n",
      "1   http://www.politifact.com   \n",
      "2   http://www.politifact.com   \n",
      "3   http://www.politifact.com   \n",
      "4   http://www.politifact.com   \n",
      "5   http://www.politifact.com   \n",
      "6   http://www.politifact.com   \n",
      "7       http://www.snopes.com   \n",
      "8       http://www.snopes.com   \n",
      "9       http://www.snopes.com   \n",
      "10  http://www.politifact.com   \n",
      "11    https://africacheck.org   \n",
      "12      http://www.snopes.com   \n",
      "13  http://www.politifact.com   \n",
      "14      http://www.snopes.com   \n",
      "15      http://www.snopes.com   \n",
      "16  http://www.politifact.com   \n",
      "17      http://www.snopes.com   \n",
      "18  http://www.politifact.com   \n",
      "19      http://www.snopes.com   \n",
      "\n",
      "                                                 link language  \n",
      "0   http://www.politifact.com/truth-o-meter/statem...  English  \n",
      "1   http://www.politifact.com/texas/statements/201...  English  \n",
      "2   http://www.politifact.com/texas/statements/201...  English  \n",
      "3   http://www.politifact.com/oregon/statements/20...  English  \n",
      "4   http://www.politifact.com/florida/statements/2...  English  \n",
      "5   http://www.politifact.com/rhode-island/stateme...  English  \n",
      "6   http://www.politifact.com/texas/statements/201...  English  \n",
      "7   https://www.snopes.com/fact-check/roger-stone-...  English  \n",
      "8   https://www.snopes.com/fact-check/muslims-rip-...  English  \n",
      "9   https://www.snopes.com/fact-check/the-soldiers...  English  \n",
      "10  http://www.politifact.com/ohio/statements/2012...  English  \n",
      "11  https://africacheck.org/reports/state-of-the-n...  English  \n",
      "12  https://www.snopes.com/fact-check/colored-only...  English  \n",
      "13  http://www.politifact.com/truth-o-meter/statem...  English  \n",
      "14  https://www.snopes.com/fact-check/death-on-the...  English  \n",
      "15    https://www.snopes.com/fact-check/shoe-stopper/  English  \n",
      "16  http://www.politifact.com/ohio/statements/2012...  English  \n",
      "17  https://www.snopes.com/fact-check/animas-river...  English  \n",
      "18  http://www.politifact.com/oregon/statements/20...  English  \n",
      "19  https://www.snopes.com/fact-check/missouri-rep...  English  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random as random\n",
    "import glob, os\n",
    "\n",
    "#concatener les fichiers en python  et lecture du fichier :\n",
    "df = pd.concat(map(pd.read_csv, glob.glob(os.path.join('', \"./DataSet/*.csv\"))))\n",
    "\n",
    "row, col = df.shape\n",
    "print(\"Size of the dataframe :\" + str(df.size) + \" (\"+str(row)+\"*\"+str(col)+\")\")\n",
    "\n",
    "\n",
    "# Afficher le nombre d'entrées NaN pour chaque colonne de notre dataframe\n",
    "print(\"\\n--------- Afficher le nombre d'entrées NaN pour chaque colonne de notre dataframe ---------\\n\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "#Afficher les différentes informations de notre dataset\n",
    "print(\"\\n--------- Afficher les différentes informations de notre dataset ---------\\n\")\n",
    "print(df.info())\n",
    "\n",
    "#Afficher les 20 premières entrées de notre dataset\n",
    "print(\"\\n--------- Afficher les 20 premières entrées de notre dataset ---------\\n\")\n",
    "print(df.head(20))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape 2 : Prétraitement du texte et traitement des valeurs manquantes\n",
    " On a fait les prétraitements suivants :\n",
    " - Transformation du texte en miniscule\n",
    " - Suppression des espaces\n",
    " - Enlever les ponctuations \n",
    " - Elimination des stopwords\n",
    " - Remplacement des mots de négation par le mot 'not'\n",
    " - Suppression des caractères non ASCII\n",
    " - Lemmatization \n",
    " - Correction orthographique \n",
    " - Conversion des nombres en mots\n",
    " - Remplacement des valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/depinfo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/depinfo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/depinfo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/depinfo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Avant Transformation des numériques en mots -------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>truthRating</th>\n",
       "      <th>ratingName</th>\n",
       "      <th>author</th>\n",
       "      <th>headline</th>\n",
       "      <th>named_entities_claim</th>\n",
       "      <th>named_entities_article</th>\n",
       "      <th>keywords</th>\n",
       "      <th>source</th>\n",
       "      <th>sourceURL</th>\n",
       "      <th>link</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/36...</td>\n",
       "      <td>' not public funding abortion legislation '</td>\n",
       "      <td>2010-03-21</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>Bart Stupak</td>\n",
       "      <td>Stupak revises abortion stance on health care ...</td>\n",
       "      <td>abortion right , barack obama , bart stupak , ...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>abortion,health care</td>\n",
       "      <td>politifact</td>\n",
       "      <td>http://www.politifact.com</td>\n",
       "      <td>http://www.politifact.com/truth-o-meter/statem...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/e6...</td>\n",
       "      <td>central health ' hospital district texas spend...</td>\n",
       "      <td>2011-03-15</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>Wayne Christian</td>\n",
       "      <td>State Rep. Wayne Christian says Central Health...</td>\n",
       "      <td>austin american-statesman , harris county hosp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abortion</td>\n",
       "      <td>politifact</td>\n",
       "      <td>http://www.politifact.com</td>\n",
       "      <td>http://www.politifact.com/texas/statements/201...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/e0...</td>\n",
       "      <td>say perry 's chiefs staff lobbyist</td>\n",
       "      <td>2010-08-14</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>Bill White</td>\n",
       "      <td>Bill White says most of Gov. Rick Perry's chie...</td>\n",
       "      <td>&amp; , bill clements , bill white , bracewell &amp; g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ethics</td>\n",
       "      <td>politifact</td>\n",
       "      <td>http://www.politifact.com</td>\n",
       "      <td>http://www.politifact.com/texas/statements/201...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/48...</td>\n",
       "      <td>say ' cochair joint way mean committee secure ...</td>\n",
       "      <td>2012-09-28</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>Mary Nolan</td>\n",
       "      <td>Did Mary Nolan secure funding for Milwaukie br...</td>\n",
       "      <td>carolyn tomei , dave hunt , fetsch , jeff merk...</td>\n",
       "      <td>Portland-Milwaukie Light Rail project</td>\n",
       "      <td>state budget,state finances,transportation</td>\n",
       "      <td>politifact</td>\n",
       "      <td>http://www.politifact.com</td>\n",
       "      <td>http://www.politifact.com/oregon/statements/20...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/80...</td>\n",
       "      <td>say gary farmer 's claim ' receive ' ' nra ' '...</td>\n",
       "      <td>2016-07-08</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>Jim Waldman</td>\n",
       "      <td>Florida Senate candidate never actually receiv...</td>\n",
       "      <td>gary farmer , gwyndolen clarke-reed , jim wald...</td>\n",
       "      <td>Gary Farmer</td>\n",
       "      <td>guns</td>\n",
       "      <td>politifact</td>\n",
       "      <td>http://www.politifact.com</td>\n",
       "      <td>http://www.politifact.com/florida/statements/2...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9995</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/cd...</td>\n",
       "      <td>kevin 's lovable yet geeky sidekick tv 's wond...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Marilyn Manson and The Wonder Years</td>\n",
       "      <td>alice cooper , behind green door , bewitch , b...</td>\n",
       "      <td>Marilyn Manson,Wonder Years</td>\n",
       "      <td>artists, asp article, music, radio &amp; tv, telev...</td>\n",
       "      <td>snopes</td>\n",
       "      <td>http://www.snopes.com</td>\n",
       "      <td>https://www.snopes.com/fact-check/wondering-ab...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9996</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/7f...</td>\n",
       "      <td>email reproduces george carlin 's list new rul...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>George Carlin New Rules for 2006</td>\n",
       "      <td>george carlin , hbo , real time bill maher , i...</td>\n",
       "      <td>George Carlin</td>\n",
       "      <td>asp article</td>\n",
       "      <td>snopes</td>\n",
       "      <td>http://www.snopes.com</td>\n",
       "      <td>https://www.snopes.com/fact-check/new-rules-fo...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9997</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/72...</td>\n",
       "      <td>' obamacare medical code confirm execution beh...</td>\n",
       "      <td>2013-11-23</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Bloggers</td>\n",
       "      <td>Bloggers say Obamacare coding system could ush...</td>\n",
       "      <td>american medical association , center disease ...</td>\n",
       "      <td>Medical Codes</td>\n",
       "      <td>health care,legal issues,public health</td>\n",
       "      <td>politifact</td>\n",
       "      <td>http://www.politifact.com</td>\n",
       "      <td>http://www.politifact.com/truth-o-meter/statem...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9998</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/7c...</td>\n",
       "      <td>wrestler john cena die car accident july two t...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>John Cena Death Hoax</td>\n",
       "      <td>facebook , interstate eighty , john cena , wwe...</td>\n",
       "      <td>John Cena</td>\n",
       "      <td>death hoax, john cena, john cena dead</td>\n",
       "      <td>snopes</td>\n",
       "      <td>http://www.snopes.com</td>\n",
       "      <td>https://www.snopes.com/fact-check/john-cena-de...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9999</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/2d...</td>\n",
       "      <td>missouri one ' thirteen state year see unemplo...</td>\n",
       "      <td>2017-11-29</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Trump misses mark on Missouri unemployment rate</td>\n",
       "      <td>bureau labor statistic , donald trump , white ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>economy,jobs</td>\n",
       "      <td>politifact</td>\n",
       "      <td>http://www.politifact.com</td>\n",
       "      <td>http://www.politifact.com/missouri/statements/...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14452 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     id  \\\n",
       "0     http://data.gesis.org/claimskg/claim_review/36...   \n",
       "1     http://data.gesis.org/claimskg/claim_review/e6...   \n",
       "2     http://data.gesis.org/claimskg/claim_review/e0...   \n",
       "3     http://data.gesis.org/claimskg/claim_review/48...   \n",
       "4     http://data.gesis.org/claimskg/claim_review/80...   \n",
       "...                                                 ...   \n",
       "9995  http://data.gesis.org/claimskg/claim_review/cd...   \n",
       "9996  http://data.gesis.org/claimskg/claim_review/7f...   \n",
       "9997  http://data.gesis.org/claimskg/claim_review/72...   \n",
       "9998  http://data.gesis.org/claimskg/claim_review/7c...   \n",
       "9999  http://data.gesis.org/claimskg/claim_review/2d...   \n",
       "\n",
       "                                                   text        date  \\\n",
       "0           ' not public funding abortion legislation '  2010-03-21   \n",
       "1     central health ' hospital district texas spend...  2011-03-15   \n",
       "2                    say perry 's chiefs staff lobbyist  2010-08-14   \n",
       "3     say ' cochair joint way mean committee secure ...  2012-09-28   \n",
       "4     say gary farmer 's claim ' receive ' ' nra ' '...  2016-07-08   \n",
       "...                                                 ...         ...   \n",
       "9995  kevin 's lovable yet geeky sidekick tv 's wond...     Unknown   \n",
       "9996  email reproduces george carlin 's list new rul...     Unknown   \n",
       "9997  ' obamacare medical code confirm execution beh...  2013-11-23   \n",
       "9998  wrestler john cena die car accident july two t...     Unknown   \n",
       "9999  missouri one ' thirteen state year see unemplo...  2017-11-29   \n",
       "\n",
       "      truthRating  ratingName           author  \\\n",
       "0               3        True      Bart Stupak   \n",
       "1               3        True  Wayne Christian   \n",
       "2               3        True       Bill White   \n",
       "3               3        True       Mary Nolan   \n",
       "4               3        True      Jim Waldman   \n",
       "...           ...         ...              ...   \n",
       "9995            1       False          Unknown   \n",
       "9996            1       False          Unknown   \n",
       "9997            1       False         Bloggers   \n",
       "9998            1       False          Unknown   \n",
       "9999            1       False     Donald Trump   \n",
       "\n",
       "                                               headline  \\\n",
       "0     Stupak revises abortion stance on health care ...   \n",
       "1     State Rep. Wayne Christian says Central Health...   \n",
       "2     Bill White says most of Gov. Rick Perry's chie...   \n",
       "3     Did Mary Nolan secure funding for Milwaukie br...   \n",
       "4     Florida Senate candidate never actually receiv...   \n",
       "...                                                 ...   \n",
       "9995                Marilyn Manson and The Wonder Years   \n",
       "9996                   George Carlin New Rules for 2006   \n",
       "9997  Bloggers say Obamacare coding system could ush...   \n",
       "9998                               John Cena Death Hoax   \n",
       "9999    Trump misses mark on Missouri unemployment rate   \n",
       "\n",
       "                                   named_entities_claim  \\\n",
       "0     abortion right , barack obama , bart stupak , ...   \n",
       "1     austin american-statesman , harris county hosp...   \n",
       "2     & , bill clements , bill white , bracewell & g...   \n",
       "3     carolyn tomei , dave hunt , fetsch , jeff merk...   \n",
       "4     gary farmer , gwyndolen clarke-reed , jim wald...   \n",
       "...                                                 ...   \n",
       "9995  alice cooper , behind green door , bewitch , b...   \n",
       "9996  george carlin , hbo , real time bill maher , i...   \n",
       "9997  american medical association , center disease ...   \n",
       "9998  facebook , interstate eighty , john cena , wwe...   \n",
       "9999  bureau labor statistic , donald trump , white ...   \n",
       "\n",
       "                     named_entities_article  \\\n",
       "0                                  abortion   \n",
       "1                                       NaN   \n",
       "2                                       NaN   \n",
       "3     Portland-Milwaukie Light Rail project   \n",
       "4                               Gary Farmer   \n",
       "...                                     ...   \n",
       "9995            Marilyn Manson,Wonder Years   \n",
       "9996                          George Carlin   \n",
       "9997                          Medical Codes   \n",
       "9998                              John Cena   \n",
       "9999                                    NaN   \n",
       "\n",
       "                                               keywords      source  \\\n",
       "0                                  abortion,health care  politifact   \n",
       "1                                              abortion  politifact   \n",
       "2                                                ethics  politifact   \n",
       "3            state budget,state finances,transportation  politifact   \n",
       "4                                                  guns  politifact   \n",
       "...                                                 ...         ...   \n",
       "9995  artists, asp article, music, radio & tv, telev...      snopes   \n",
       "9996                                        asp article      snopes   \n",
       "9997             health care,legal issues,public health  politifact   \n",
       "9998              death hoax, john cena, john cena dead      snopes   \n",
       "9999                                       economy,jobs  politifact   \n",
       "\n",
       "                      sourceURL  \\\n",
       "0     http://www.politifact.com   \n",
       "1     http://www.politifact.com   \n",
       "2     http://www.politifact.com   \n",
       "3     http://www.politifact.com   \n",
       "4     http://www.politifact.com   \n",
       "...                         ...   \n",
       "9995      http://www.snopes.com   \n",
       "9996      http://www.snopes.com   \n",
       "9997  http://www.politifact.com   \n",
       "9998      http://www.snopes.com   \n",
       "9999  http://www.politifact.com   \n",
       "\n",
       "                                                   link language  \n",
       "0     http://www.politifact.com/truth-o-meter/statem...  English  \n",
       "1     http://www.politifact.com/texas/statements/201...  English  \n",
       "2     http://www.politifact.com/texas/statements/201...  English  \n",
       "3     http://www.politifact.com/oregon/statements/20...  English  \n",
       "4     http://www.politifact.com/florida/statements/2...  English  \n",
       "...                                                 ...      ...  \n",
       "9995  https://www.snopes.com/fact-check/wondering-ab...  English  \n",
       "9996  https://www.snopes.com/fact-check/new-rules-fo...  English  \n",
       "9997  http://www.politifact.com/truth-o-meter/statem...  English  \n",
       "9998  https://www.snopes.com/fact-check/john-cena-de...  English  \n",
       "9999  http://www.politifact.com/missouri/statements/...  English  \n",
       "\n",
       "[14452 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#commencement des prétraitement :\n",
    "\n",
    "#import nécessaire :\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords') \n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import unicodedata\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "import inflect\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "\n",
    "# Fonctions crées:\n",
    "def nombreversmot(text): #Conversion des nombres vers mots\n",
    "    if text.isdigit():\n",
    "        return p.number_to_words(text)\n",
    "    else: return text\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def nltk2wn_tag(nltk_tag): #Lemmatization de tous type de mots\n",
    "  if nltk_tag.startswith('J'):\n",
    "    return wordnet.ADJ\n",
    "  elif nltk_tag.startswith('V'):\n",
    "    return wordnet.VERB\n",
    "  elif nltk_tag.startswith('N'):\n",
    "    return wordnet.NOUN\n",
    "  elif nltk_tag.startswith('R'):\n",
    "    return wordnet.ADV\n",
    "  else:                    \n",
    "    return None\n",
    "def lemmatize_sentence(sentence):\n",
    "  nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))    \n",
    "  wn_tagged = map(lambda x: (x[0], nltk2wn_tag(x[1])), nltk_tagged)\n",
    "  res_words = []\n",
    "  for word, tag in wn_tagged:\n",
    "    if tag is None:                        \n",
    "      res_words.append(word)\n",
    "    else:\n",
    "      res_words.append(lemmatizer.lemmatize(word, tag))\n",
    "  return \" \".join(res_words)\n",
    "\n",
    "# ------------------------------ Prétraitements sur la colonne text ----------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# --- La mise en miniscule ---\n",
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "\n",
    "# --- White spaces removal --- \n",
    "df['text'] = df['text'].str.strip()\n",
    "\n",
    "# --- Elimination des stopWord  --- \n",
    "\n",
    "stop_words = set(stopwords.words(\"english\")) #La liste des stopwords en Anglais\n",
    "negation ={'haven''t','cannot',\"doesn't\",\"shouldn't\",\"needn't\",\"shant't\",\"weren't\",\"hasn't\", \"wasn't\",\"didn't\", \"aren't\",'not', \"mightn't\", \"mustn't\", 'no',  \"wouldn't\", \"mightn't\", \"won't\",  \"needn't\", \"wasn't\", \"wouldn't\",  \"isn't\", \"doesn't\", \"weren't\", \"isn't\", \"hasn't\", \"hadn't\", \"don't\", \"hadn't\",\"couldn't\"} #La liste des stopwords de négation en Anglais  \n",
    "a= df['text'].str.replace(\"’\",\"'\") #Extraire toutes les entrées de la colonne text\n",
    "pat = r'\\b(?:{})\\b'.format('|'.join(negation))\n",
    "a=a.str.replace(pat,'not')\n",
    "pat='\\w*\\d\\w*'\n",
    "text_without_stopwords=[] #Une liste dont on va affecter les textes après l'élimination des stop words\n",
    "tk=TweetTokenizer()\n",
    "for text in a.iteritems(): #On parcourt toutes les lignes\n",
    " tokens = tk.tokenize(str(text[1]))\n",
    " result = [i for i in tokens if not i in stop_words-negation]\n",
    " splitor=\" \"\n",
    " concatinated = splitor.join(result) #concatiner les tokens\n",
    " text_without_stopwords.append(concatinated)\n",
    "\n",
    "df['text']=text_without_stopwords\n",
    "\n",
    "# --- Suppression des ponctuations  [!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]  --- \n",
    "\n",
    "text_without_punctuation=[] # pour y mettre notre résultat\n",
    "a= df['text'] #Extraire toutes les entrées de la colonne text deja traiter pour faire la suite\n",
    "tk=TweetTokenizer()\n",
    "for text in a.iteritems(): #On parcourt toutes les lignes\n",
    "    result = re.sub('[!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]', '', str(text[1]) )\n",
    "    splitor=\"\" #séparateur de mots\n",
    "    concatinated = splitor.join(result) #concatiner les résultats\n",
    "    text_without_punctuation.append(concatinated)\n",
    "\n",
    "df['text']=text_without_punctuation\n",
    "\n",
    "#  --- Suppression des caractères non ASCII  --- \n",
    "text_without_ascii=[] # pour y mettre notre résultat\n",
    "a= df['text'] #Extraire toutes les entrées de la colonne text deja traiter pour faire la suite\n",
    "tk=TweetTokenizer()\n",
    "for text in a.iteritems(): #On parcourt toutes les lignes\n",
    "    text = unicodedata.normalize('NFKD', str(text[1]) ).encode(\"ascii\", \"ignore\").decode(\"utf-8\", 'ignore')\n",
    "    splitor=\"\" #séparateur de mots\n",
    "    concatinated = splitor.join(text)\n",
    "    text_without_ascii.append(concatinated)\n",
    "\n",
    "df['text']=text_without_ascii\n",
    "\n",
    "#  --- Conversion des nombres en mots   --- \n",
    "print(\"---------- Avant Transformation des numériques en mots -------------------\")\n",
    "numbertransf=[] # pour y mettre notre résultat\n",
    "p = inflect.engine()\n",
    "a= df['text'] #Extraire toutes les entrées de la colonne text deja traiter pour faire la suite\n",
    "tk=TweetTokenizer()\n",
    "for text in a.iteritems():\n",
    "     tokens = tk.tokenize(str(text[1]))\n",
    "     result = [nombreversmot(i) for i in tokens]\n",
    "     splitor=\" \"\n",
    "     concatinated = splitor.join(result) #concatiner les tokens\n",
    "     numbertransf.append(concatinated)\n",
    "df['text']=numbertransf\n",
    "df['text'] = df['text'].str.strip()\n",
    "\n",
    "#  --- Lemmatization --- \n",
    "text_lemmatizer=[] # pour y mettre notre résultat\n",
    "a= df['text'] #Extraire toutes les entrées de la colonne text deja traiter pour faire la suite\n",
    "tk=TweetTokenizer()\n",
    "for ligne in a.iteritems(): #On parcourt toutes les lignes\n",
    "    newList=[]\n",
    "    newList.append(lemmatize_sentence(ligne[1])) #lemmatisation\n",
    "    splitor=\" \" #séparateur de mots\n",
    "    concatinated = splitor.join(newList)\n",
    "    text_lemmatizer.append(concatinated)\n",
    "df['text']=text_lemmatizer\n",
    "\n",
    "# #suppression des common word: (ANNULEE CAR CA PRENDS BEAUCOUP DE TEMPS ET CA CHANGE PAS BEACOUP DE CHOSES)\n",
    "# word_counter  = Counter()\n",
    "# for sentence in df[\"text\"].values:\n",
    "#     for word in sentence.split():\n",
    "#         word_counter[word] += 1\n",
    "# most = word_counter.most_common(10)\n",
    "# print(\"most common word\"+str(most))\n",
    "# print(\"Suppression of the common word de nos artiles : \")\n",
    "# most_word = set([w for (w, wc) in most])\n",
    "# def delmost_word(sentence):\n",
    "#     return \" \".join([word for word in str(sentence).split() if word not in most_word])\n",
    "# df[\"text\"] = df[\"text\"].apply(delmost_word)\n",
    "# df[\"text\"].head()\n",
    "\n",
    "# ------------------------------ FIN prétraitements sur la colonne text ----------------------------------------\n",
    "\n",
    "# ------------------------------ Prétraitements sur la colonne named_entities_claim ----------------------------\n",
    "\n",
    "# --- La mise en miniscule ---\n",
    "df['named_entities_claim']=df['named_entities_claim'].str.strip()\n",
    "\n",
    "# --- White spaces removal --- \n",
    "df['named_entities_claim']=df['named_entities_claim'].str.lower()\n",
    "\n",
    "# --- Le remplacement des valeurs manquantes par 'nan' ---\n",
    "df['named_entities_claim'] = df['named_entities_claim'].fillna('nan')\n",
    "\n",
    "# --- Elimination des stopWord  --- \n",
    "a= df['named_entities_claim'].str.replace(\"’\",\"'\")\n",
    "text_without_stopwords=[]\n",
    "tk=TweetTokenizer()\n",
    "for text in a.iteritems(): \n",
    " tokens = tk.tokenize(str(text[1]))\n",
    " result = [i for i in tokens if not i in stop_words-negation]\n",
    " splitor=\" \"\n",
    " concatinated = splitor.join(result)\n",
    " text_without_stopwords.append(concatinated)\n",
    "df['named_entities_claim']=text_without_stopwords\n",
    "\n",
    "#  --- Conversion des nombres en mots   --- \n",
    "numbertransf=[]\n",
    "p = inflect.engine()\n",
    "a= df['named_entities_claim'] \n",
    "tk=TweetTokenizer()\n",
    "for text in a.iteritems():\n",
    "     tokens = tk.tokenize(str(text[1]))\n",
    "     result = [nombreversmot(i) for i in tokens]\n",
    "     splitor=\" \"\n",
    "     concatinated = splitor.join(result) #concatiner les tokens\n",
    "     numbertransf.append(concatinated)\n",
    "df['named_entities_claim']=numbertransf\n",
    "df['named_entities_claim'] = df['named_entities_claim'].str.strip()\n",
    "\n",
    "#  --- Lemmatization --- \n",
    "text_lemmatizer=[] # pour y mettre notre résultat\n",
    "a= df['named_entities_claim'] #Extraire toutes les entrées de la colonne text deja traiter pour faire la suite\n",
    "tk=TweetTokenizer()\n",
    "for ligne in a.iteritems(): #On parcourt toutes les lignes\n",
    "    newList=[]\n",
    "    newList.append(lemmatize_sentence(ligne[1])) #lemmatisation\n",
    "    splitor=\" \" #séparateur de mots\n",
    "    concatinated = splitor.join(newList)\n",
    "    text_lemmatizer.append(concatinated)\n",
    "df['named_entities_claim']=text_lemmatizer\n",
    "\n",
    "# ------------------------------ FIN prétraitements sur la colonne named_entities_claim -----------------------\n",
    "\n",
    "# ------------------------------ Prétraitements sur la colonne keyword ----------------------------------------\n",
    "\n",
    "\n",
    "df['keywords'] = df['keywords'].str.lower()\n",
    "\n",
    "\n",
    "#White spaces removal\n",
    "df['keywords'] = df['keywords'].str.strip()\n",
    "df['keywords'] = df['keywords'].fillna('nan')\n",
    "\n",
    "\n",
    "# ------------------------------ FIN prétraitements sur la colonne keyword ------------------------------------\n",
    "\n",
    "# df['headline']=df['headline'].str.strip()\n",
    "# df['headline']=df['headline'].str.lower()\n",
    "# df['headline'] = df['headline'].fillna('nan')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #Lemmatization :\n",
    "# text_lemmatizer=[] # pour y mettre notre résultat\n",
    "# a= df['headline'] #Extraire toutes les entrées de la colonne text deja traiter pour faire la suite\n",
    "# tk=TweetTokenizer()\n",
    "# for ligne in a.iteritems(): #On parcourt toutes les lignes\n",
    "#     newList=[]\n",
    "#     newList.append(lemmatize_sentence(ligne[1])) #lemmatisation\n",
    "#     splitor=\" \" #séparateur de mots\n",
    "#     concatinated = splitor.join(newList)\n",
    "#     text_lemmatizer.append(concatinated)\n",
    "# df['headline']=text_lemmatizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df\n",
    "\n",
    "#Fin des prétraitement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape 3 : Conversion des textes en valeurs numériques\n",
    " On transforme nos données de textes en valeurs numériques, en utilisant des LabelEncoder sur les colonnes qu'on a  sauf la colonne texte et la colonne keyword, dont on a essayé Tf-IDF pour les transformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       and  say  thousand  two\n",
      "0        0    0         0    0\n",
      "1        0    0         0    0\n",
      "2        0    1         0    0\n",
      "3        0    1         0    0\n",
      "4        0    1         0    0\n",
      "...    ...  ...       ...  ...\n",
      "14447    0    0         0    0\n",
      "14448    1    0         1    1\n",
      "14449    0    0         0    0\n",
      "14450    1    0         1    1\n",
      "14451    0    0         0    0\n",
      "\n",
      "[14452 rows x 4 columns]\n",
      "       barack  donald  facebook  house  http  internet  john  national  new  \\\n",
      "0           0       0         0      0     0         0     0         0    0   \n",
      "1           0       0         0      0     0         0     0         0    0   \n",
      "2           0       0         0      0     0         0     0         0    0   \n",
      "3           0       0         0      0     0         0     0         0    0   \n",
      "4           0       0         0      0     0         0     0         0    0   \n",
      "...       ...     ...       ...    ...   ...       ...   ...       ...  ...   \n",
      "14447       0       0         0      0     0         0     0         0    0   \n",
      "14448       0       0         0      0     0         0     0         0    1   \n",
      "14449       0       0         0      0     0         0     0         0    0   \n",
      "14450       0       0         0      0     0         0     1         0    0   \n",
      "14451       0       0         0      0     0         0     0         0    0   \n",
      "\n",
      "       news  ...  politifact  republican  state  time  trump  twitter  \\\n",
      "0         0  ...           0           0      0     0      0        0   \n",
      "1         0  ...           0           0      0     0      0        0   \n",
      "2         0  ...           0           0      0     0      0        0   \n",
      "3         0  ...           0           0      0     0      0        0   \n",
      "4         0  ...           0           0      0     0      0        0   \n",
      "...     ...  ...         ...         ...    ...   ...    ...      ...   \n",
      "14447     0  ...           0           0      0     0      0        0   \n",
      "14448     0  ...           0           0      0     0      0        0   \n",
      "14449     0  ...           0           0      0     0      0        0   \n",
      "14450     0  ...           0           0      0     0      0        0   \n",
      "14451     0  ...           0           0      2     0      0        0   \n",
      "\n",
      "       university  washington  white  york  \n",
      "0               0           0      0     0  \n",
      "1               0           0      0     0  \n",
      "2               0           0      0     0  \n",
      "3               0           0      0     0  \n",
      "4               0           0      0     0  \n",
      "...           ...         ...    ...   ...  \n",
      "14447           0           0      0     0  \n",
      "14448           0           0      0     0  \n",
      "14449           0           0      0     0  \n",
      "14450           0           0      0     0  \n",
      "14451           0           0      0     0  \n",
      "\n",
      "[14452 rows x 21 columns]\n",
      "\n",
      "Ajout des colonne au dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/depinfo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:66: FutureWarning: The join_axes-keyword is deprecated. Use .reindex or .reindex_like on the result to achieve the same functionality.\n",
      "/home/depinfo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:67: FutureWarning: The join_axes-keyword is deprecated. Use .reindex or .reindex_like on the result to achieve the same functionality.\n",
      "/home/depinfo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:68: FutureWarning: The join_axes-keyword is deprecated. Use .reindex or .reindex_like on the result to achieve the same functionality.\n",
      "/home/depinfo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:69: FutureWarning: The join_axes-keyword is deprecated. Use .reindex or .reindex_like on the result to achieve the same functionality.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#np.random.seed(500) #utilisé pour avoir le même résultat à chaque exécution \n",
    "#On crée des variable LabelEncoder qui vont servir à transférer nos données en valeurs numériques\n",
    "l1=LabelEncoder()\n",
    "l2=LabelEncoder()\n",
    "l3=LabelEncoder()\n",
    "l4=LabelEncoder()\n",
    "l5=LabelEncoder()\n",
    "l6=LabelEncoder()\n",
    "l7=LabelEncoder()\n",
    "l8=LabelEncoder()\n",
    "l9=LabelEncoder()\n",
    "l10=LabelEncoder()\n",
    "l11=LabelEncoder()\n",
    "l12=LabelEncoder()\n",
    "l13=LabelEncoder()\n",
    "df=df.applymap(str) #on transforme tous nous données en String car y'avais des entrées qui ont une combinaison du string et float\n",
    "#On applique la mesure TF-IDF sur la colonne text et la colonne keywords\n",
    "# Tfidf_vect = TfidfVectorizer(max_features=40000,min_df=0.03, max_df=0.7)\n",
    "# tfidfx=Tfidf_vect.fit_transform(df['named_entities_claim'])\n",
    "# df1 = pd.DataFrame(tfidfx.toarray(), columns=Tfidf_vect.get_feature_names())\n",
    "# Tfidf_vect = TfidfVectorizer(max_features=4000)\n",
    "# print(df['named_entities_claim'])\n",
    "cd = CountVectorizer(min_df=0.1)\n",
    "cd.fit(df[\"text\"])\n",
    "tfidfx=cd.transform(df['text'])\n",
    "df2 = pd.DataFrame(tfidfx.toarray(), columns=cd.get_feature_names())\n",
    "print(df2)\n",
    "cd = CountVectorizer(min_df=0.1)\n",
    "cd.fit(df[\"named_entities_claim\"])\n",
    "tfidfx=cd.transform(df['text'])\n",
    "df1 = pd.DataFrame(tfidfx.toarray(), columns=cd.get_feature_names())\n",
    "print(df1)\n",
    "\n",
    "# cd = CountVectorizer(min_df=0.1)\n",
    "# cd.fit(df[\"headline\"])\n",
    "# tfidfx=cd.transform(df['text'])\n",
    "# df3 = pd.DataFrame(tfidfx.toarray(), columns=cd.get_feature_names())\n",
    "# print(df3)\n",
    "\n",
    "#on transfère toutes les valeurs des colonnes qu'on va utiliser en valeurs numériques \n",
    "df['id']=l1.fit_transform(df['id'])\n",
    "df['text']=l3.fit_transform(df['text'])\n",
    "df['date']=l3.fit_transform(df['date'])\n",
    "df['author']=l5.fit_transform(df['author'])\n",
    "df['headline']=l6.fit_transform(df['headline'])\n",
    "# df['named_entities_claim']=l7.fit_transform(df['named_entities_claim'])\n",
    "df['named_entities_article']=l8.fit_transform(df['named_entities_article'])\n",
    "df['sourceURL']=l11.fit_transform(df['sourceURL'])\n",
    "df['link']=l12.fit_transform(df['link'])\n",
    "df['language']=l13.fit_transform(df['language'])\n",
    "df['ratingName']=l13.fit_transform(df['ratingName'])\n",
    "print (\"\\nAjout des colonne au dataframe\")\n",
    "dummies = pd.get_dummies(df['source'],prefix ='Src')\n",
    "\n",
    "\n",
    "\n",
    "df= pd.concat([df, df1], axis=1,join_axes=[df.index])\n",
    "df=pd.concat([df, df2], axis=1,join_axes=[df.index])\n",
    "df= pd.concat([df, dummies], axis=1,join_axes=[df.index])\n",
    "df1=pd.concat([df1, df2], axis=1,join_axes=[df1.index])\n",
    "# df=pd.concat([df, df3], axis=1,join_axes=[df.index])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape 4 : Dévision du jeu de données en données d'entraînement et données de test\n",
    "On mélange le dataframe qu'on a et après on sélectionne 80% des données pour l'entraînement et 20% pour le test, ensuite on sélectionne les colonnes features dont on s'intéresse lors du classification, et on essaye plusieurs combinaisons de colonnes pour arriver à une meilleure accuracy (#TODO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Src_africacheck', 'Src_factscan', 'Src_politifact', 'Src_snopes', 'Src_truthorfiction', 'barack', 'donald', 'facebook', 'house', 'http', 'internet', 'john', 'national', 'new', 'news', 'obama', 'politifact', 'republican', 'state', 'time', 'trump', 'twitter', 'university', 'washington', 'white', 'york', 'and', 'say', 'thousand', 'two']\n",
      "Accuracy: 0.30231753718436527\n",
      "Accuracy: 0.7014873746108613\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "##### \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "X_train, X_test ,y_train, y_test = train_test_split(df,df[\"truthRating\"], test_size=0.2, random_state=int(time.time()))\n",
    "\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "used_features =['Src_africacheck', 'Src_factscan', 'Src_politifact', 'Src_snopes', 'Src_truthorfiction']\n",
    "\n",
    "for col in df1.columns :\n",
    "    used_features.append(col)\n",
    "\n",
    "\n",
    "print(used_features)\n",
    "gnb.fit(\n",
    "    X_train[used_features].values,\n",
    "    y_train\n",
    ")\n",
    "mnb=MultinomialNB()\n",
    "mnb.fit(X_train[used_features].values,\n",
    "    y_train)\n",
    "\n",
    "y_pred = gnb.predict(X_test[used_features])\n",
    "mnbpred=mnb.predict(X_test[used_features])\n",
    "# print(\"Number of mislabeled points out of a total {} points : {}, performance {:05.2f}%\"\n",
    "#       .format(\n",
    "#           X_test.shape[0],\n",
    "#           (X_test[\"ratingName\"] != y_pred).sum(),\n",
    "#           100*(1-(X_test[\"ratingName\"] != y_pred).sum()/X_test.shape[0])\n",
    "# ))\n",
    "# print(df['ratingName'])\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, mnbpred))\n",
    "print(len(used_features))##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape 5 :  Entraînement et prédiction en utilisant des modèles de \n",
    "\n",
    "\n",
    "classification\n",
    "On passe les features et targets des données d'entraînement à nos classifiers (#TODO tester plusieurs classifieurs) et après on lui laisse prédire les valeurs des données de test (soit vrai, faux ou mixture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin by importing all necessary libraries\n",
    "# import pandas as pd\n",
    "# import time\n",
    "# from sklearn.utils import shuffle\n",
    "# import numpy as np\n",
    "# ##### \n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "\n",
    "# #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=int(time.time()))\n",
    "# label = df[\"ratingName\"]\n",
    "# features=zip(df[\"date\"],df[\"author\"],df[\"named_entities_claim\"],df[\"source\"],df[\"sourceURL\"])\n",
    "# # dt = df[\"date\"]\n",
    "# # src = df[\"sourceURL\"]\n",
    "# # features=zip(dt,src)\n",
    "\n",
    "# pip\n",
    "\n",
    "# #print(df[\"author\"])  \n",
    "# print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
