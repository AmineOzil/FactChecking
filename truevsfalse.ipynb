{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Fact-checking project using pandas and ScikitLearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travail réalisé par : Bouali Mohammed-Amin, Oussama Nassim Sehout, Chahinez Benallal, Abdellah Choukri \n",
    "\n",
    "\n",
    "\n",
    "###  Sommaire du travail :\n",
    " - Préparation de l'environnement du travail\n",
    " - Chargement et analyse du jeu de données\n",
    " - Prétraitement du texte et traitement des valeurs manquantes\n",
    " - Conversion des textes aux valeurs numériques\n",
    " - Division du jeu de données en données d'entraînement et données de test\n",
    " - Entraînement et prédiction en utilisant des modèles de classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape 0: Préparation de l'environnement du travail \n",
    "\n",
    "- pip install inflect\n",
    "- pip install contractions (on a commenté cette partie du prétraitement donc ce n'est plus nécessaire)\n",
    "- pip install autocorrect  (on a commenté cette partie du prétraitement donc ce n'est plus nécessaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape 1 : Chargement et analyse du jeu de données  \n",
    "  On a généré deux fichiers csv, un avec valeurs true, et l'autre avec valeurs false à partir du site ClaimsKg, après on les concatène pour avoir notre jeu de données, ensuite on affiche les informations nécessaires qui peuvent nous aider à bien comprendre de quoi notre jeu de données est constitué."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataframe :202328 (14452*14)\n",
      "\n",
      "--------- Afficher le nombre d'entrées NaN pour chaque colonne de notre dataframe ---------\n",
      "\n",
      "id                           0\n",
      "text                         0\n",
      "date                         0\n",
      "truthRating                  0\n",
      "ratingName                   0\n",
      "author                       0\n",
      "headline                     0\n",
      "named_entities_claim        27\n",
      "named_entities_article    4865\n",
      "keywords                   973\n",
      "source                       0\n",
      "sourceURL                    0\n",
      "link                         0\n",
      "language                     0\n",
      "dtype: int64\n",
      "\n",
      "--------- Afficher les différentes informations de notre dataset ---------\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 14452 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      "id                        14452 non-null object\n",
      "text                      14452 non-null object\n",
      "date                      14452 non-null object\n",
      "truthRating               14452 non-null int64\n",
      "ratingName                14452 non-null bool\n",
      "author                    14452 non-null object\n",
      "headline                  14452 non-null object\n",
      "named_entities_claim      14425 non-null object\n",
      "named_entities_article    9587 non-null object\n",
      "keywords                  13479 non-null object\n",
      "source                    14452 non-null object\n",
      "sourceURL                 14452 non-null object\n",
      "link                      14452 non-null object\n",
      "language                  14452 non-null object\n",
      "dtypes: bool(1), int64(1), object(12)\n",
      "memory usage: 1.6+ MB\n",
      "None\n",
      "\n",
      "--------- Afficher les 20 premières entrées de notre dataset ---------\n",
      "\n",
      "                                                   id  \\\n",
      "0   http://data.gesis.org/claimskg/claim_review/36...   \n",
      "1   http://data.gesis.org/claimskg/claim_review/e6...   \n",
      "2   http://data.gesis.org/claimskg/claim_review/e0...   \n",
      "3   http://data.gesis.org/claimskg/claim_review/48...   \n",
      "4   http://data.gesis.org/claimskg/claim_review/80...   \n",
      "5   http://data.gesis.org/claimskg/claim_review/2c...   \n",
      "6   http://data.gesis.org/claimskg/claim_review/3a...   \n",
      "7   http://data.gesis.org/claimskg/claim_review/90...   \n",
      "8   http://data.gesis.org/claimskg/claim_review/09...   \n",
      "9   http://data.gesis.org/claimskg/claim_review/7a...   \n",
      "10  http://data.gesis.org/claimskg/claim_review/58...   \n",
      "11  http://data.gesis.org/claimskg/claim_review/c4...   \n",
      "12  http://data.gesis.org/claimskg/claim_review/00...   \n",
      "13  http://data.gesis.org/claimskg/claim_review/97...   \n",
      "14  http://data.gesis.org/claimskg/claim_review/66...   \n",
      "15  http://data.gesis.org/claimskg/claim_review/86...   \n",
      "16  http://data.gesis.org/claimskg/claim_review/0f...   \n",
      "17  http://data.gesis.org/claimskg/claim_review/4c...   \n",
      "18  http://data.gesis.org/claimskg/claim_review/a4...   \n",
      "19  http://data.gesis.org/claimskg/claim_review/28...   \n",
      "\n",
      "                                                 text        date  \\\n",
      "0   'There will be no public funding for abortion ...  2010-03-21   \n",
      "1   Central Health 'is the only hospital district ...  2011-03-15   \n",
      "2   Says most of Perry's chiefs of staff have been...  2010-08-14   \n",
      "3   Says 'as Co-Chair of the Joint Ways & Means Co...  2012-09-28   \n",
      "4   Says Gary Farmer’s claim that he 'received an ...  2016-07-08   \n",
      "5   Democrats say Chafee wants to tax equipment th...  2010-07-23   \n",
      "6   Will Hurd 'tends to have a 96 percent straight...  2015-10-23   \n",
      "7   Roger Stone once marketed 'Hillary for Prison'...     Unknown   \n",
      "8   A video shows a group of Muslims ripping up an...     Unknown   \n",
      "9   A U.S. serviceman wrote a poem describing a so...     Unknown   \n",
      "10  Says Republicans supported legislation on earl...  2012-08-15   \n",
      "11  ”Since its inception 3.881 million tonnes of c...     Unknown   \n",
      "12  During the Jim Crow era of racial segregation ...     Unknown   \n",
      "13  'Our businesses have created jobs every single...  2016-01-12   \n",
      "14  A survivor of the September 11 terrorist attac...     Unknown   \n",
      "15  Condoleezza Rice shopped for expensive shoes d...     Unknown   \n",
      "16  The Buffett Rule 'will bring in less than $5 b...  2012-04-16   \n",
      "17  A spill by the Environmental Protection Agency...     Unknown   \n",
      "18  Says he 'is the only candidate in the race for...  2012-05-04   \n",
      "19  Missouri Republicans passed a bill to lower th...     Unknown   \n",
      "\n",
      "    truthRating  ratingName                            author  \\\n",
      "0             3        True                       Bart Stupak   \n",
      "1             3        True                   Wayne Christian   \n",
      "2             3        True                        Bill White   \n",
      "3             3        True                        Mary Nolan   \n",
      "4             3        True                       Jim Waldman   \n",
      "5             3        True  Democratic Governors Association   \n",
      "6             3        True                      Pete Gallego   \n",
      "7             3        True                           Unknown   \n",
      "8             3        True                           Unknown   \n",
      "9             3        True                           Unknown   \n",
      "10            3        True                      Teresa Fedor   \n",
      "11            3        True                           Unknown   \n",
      "12            3        True                           Unknown   \n",
      "13            3        True                      Barack Obama   \n",
      "14            3        True                           Unknown   \n",
      "15            3        True                           Unknown   \n",
      "16            3        True                       Rob Portman   \n",
      "17            3        True                           Unknown   \n",
      "18            3        True                       John Ludlow   \n",
      "19            3        True                           Unknown   \n",
      "\n",
      "                                             headline  \\\n",
      "0   Stupak revises abortion stance on health care ...   \n",
      "1   State Rep. Wayne Christian says Central Health...   \n",
      "2   Bill White says most of Gov. Rick Perry's chie...   \n",
      "3   Did Mary Nolan secure funding for Milwaukie br...   \n",
      "4   Florida Senate candidate never actually receiv...   \n",
      "5     Democrats say Chafee would tax amputee veterans   \n",
      "6   Pete Gallego says Will Hurd votes with House R...   \n",
      "7   Did Roger Stone Sell ‘Hillary for Prison’ Merc...   \n",
      "8   Did This Video Show Muslims Ripping Up an Amer...   \n",
      "9                The Soldier’s Night Before Christmas   \n",
      "10  Teresa Fedor says Republicans supported Ohio's...   \n",
      "11  State of the Nation 2019: Did Kenyatta get his...   \n",
      "12  Did ‘No Whites Allowed’ Signs Exist in the Seg...   \n",
      "13  Business has created jobs every month since Ob...   \n",
      "14                           WTC Survivor Ferry Death   \n",
      "15                   Condoleezza Rice Shops for Shoes   \n",
      "16  Rob Portman says Buffett Rule would raise just...   \n",
      "17          Did the EPA Cause the Animas River Spill?   \n",
      "18  Is John Ludlow the only candidate in the race ...   \n",
      "19  Did Missouri Republicans Pass a Law Lowering t...   \n",
      "\n",
      "                                 named_entities_claim  \\\n",
      "0   Abortion rights,Barack Obama,Bart Stupak,Ben N...   \n",
      "1   Austin American-Statesman,Harris County Hospit...   \n",
      "2   AT&T,Bill Clements,Bill White,Bracewell & Giul...   \n",
      "3   Carolyn Tomei,Dave Hunt,Fetsch,Jeff Merkley,Ka...   \n",
      "4   Gary Farmer,Gwyndolen Clarke-Reed,Jim Waldman,...   \n",
      "5   Democratic Governors Association,John Chafee,K...   \n",
      "6   114th Congress,2016 general election,CQ Roll C...   \n",
      "7   2016 presidential campaign,Donald Trump,Hillar...   \n",
      "8   Cuba,Facebook,Guantanamo Bay,Guantánamo Bay,Gu...   \n",
      "9   Afghanistan,Christmas decoration,Japan,Lance C...   \n",
      "10  2008 election,Barack Obama,Bob Taft,Democrat,J...   \n",
      "11  2.2 million,Africa Check,Garissa,Institute of ...   \n",
      "12  African American,Augusta Chronicle,Augusta, Ge...   \n",
      "13  Affordable Care Act,Barack Obama,Bureau of Lab...   \n",
      "14  11 September 2001 terrorist attacks,American A...   \n",
      "15  Condoleezza Rice,Dr. Condoleezza Rice,Hurrican...   \n",
      "16  Buffet rule,Buffett Rule,Bush tax cuts,Congres...   \n",
      "17  Animas River,EPA,Environmental Protection Agen...   \n",
      "18  Clackamas County,Dave Hunt,Oregon Revised Stat...   \n",
      "19  Eric Greitens,Federal Reserve Bank of St. Loui...   \n",
      "\n",
      "                               named_entities_article  \\\n",
      "0                                            abortion   \n",
      "1                                                 NaN   \n",
      "2                                                 NaN   \n",
      "3               Portland-Milwaukie Light Rail project   \n",
      "4                                         Gary Farmer   \n",
      "5                                                 NaN   \n",
      "6                                Republican,Will Hurd   \n",
      "7                      Hillary for Prison,Roger Stone   \n",
      "8                                       New York City   \n",
      "9                              night before Christmas   \n",
      "10                                       early voting   \n",
      "11                                        2.2 million   \n",
      "12  American South,Jim Crow,Jim Crow era,racial se...   \n",
      "13                                                NaN   \n",
      "14  September 11 terrorist attacks,World Trade Center   \n",
      "15                 Condoleezza Rice,Hurricane Katrina   \n",
      "16                                       Buffett Rule   \n",
      "17                                       Animas River   \n",
      "18                                   Clackamas County   \n",
      "19                              Missouri,minimum wage   \n",
      "\n",
      "                                             keywords       source  \\\n",
      "0                                Abortion,Health Care   politifact   \n",
      "1                                            Abortion   politifact   \n",
      "2                                              Ethics   politifact   \n",
      "3          State Budget,State Finances,Transportation   politifact   \n",
      "4                                                Guns   politifact   \n",
      "5                                               Taxes   politifact   \n",
      "6   Bipartisanship,Congress,Redistricting,Voting R...   politifact   \n",
      "7                                                 NaN       snopes   \n",
      "8                             flag desecration, islam       snopes   \n",
      "9                                           christmas       snopes   \n",
      "10      Elections,Government regulation,Voting Record   politifact   \n",
      "11  development, economy, State of the Nation, Uhu...  africacheck   \n",
      "12                                                NaN       snopes   \n",
      "13                           Economy,Health Care,Jobs   politifact   \n",
      "14                                        ASP Article       snopes   \n",
      "15                                        ASP Article       snopes   \n",
      "16                       Deficit,Federal Budget,Taxes   politifact   \n",
      "17  animas river, environmental protection agency,...       snopes   \n",
      "18  County Budget,County Government,Message Machin...   politifact   \n",
      "19                             minimum wage, missouri       snopes   \n",
      "\n",
      "                    sourceURL  \\\n",
      "0   http://www.politifact.com   \n",
      "1   http://www.politifact.com   \n",
      "2   http://www.politifact.com   \n",
      "3   http://www.politifact.com   \n",
      "4   http://www.politifact.com   \n",
      "5   http://www.politifact.com   \n",
      "6   http://www.politifact.com   \n",
      "7       http://www.snopes.com   \n",
      "8       http://www.snopes.com   \n",
      "9       http://www.snopes.com   \n",
      "10  http://www.politifact.com   \n",
      "11    https://africacheck.org   \n",
      "12      http://www.snopes.com   \n",
      "13  http://www.politifact.com   \n",
      "14      http://www.snopes.com   \n",
      "15      http://www.snopes.com   \n",
      "16  http://www.politifact.com   \n",
      "17      http://www.snopes.com   \n",
      "18  http://www.politifact.com   \n",
      "19      http://www.snopes.com   \n",
      "\n",
      "                                                 link language  \n",
      "0   http://www.politifact.com/truth-o-meter/statem...  English  \n",
      "1   http://www.politifact.com/texas/statements/201...  English  \n",
      "2   http://www.politifact.com/texas/statements/201...  English  \n",
      "3   http://www.politifact.com/oregon/statements/20...  English  \n",
      "4   http://www.politifact.com/florida/statements/2...  English  \n",
      "5   http://www.politifact.com/rhode-island/stateme...  English  \n",
      "6   http://www.politifact.com/texas/statements/201...  English  \n",
      "7   https://www.snopes.com/fact-check/roger-stone-...  English  \n",
      "8   https://www.snopes.com/fact-check/muslims-rip-...  English  \n",
      "9   https://www.snopes.com/fact-check/the-soldiers...  English  \n",
      "10  http://www.politifact.com/ohio/statements/2012...  English  \n",
      "11  https://africacheck.org/reports/state-of-the-n...  English  \n",
      "12  https://www.snopes.com/fact-check/colored-only...  English  \n",
      "13  http://www.politifact.com/truth-o-meter/statem...  English  \n",
      "14  https://www.snopes.com/fact-check/death-on-the...  English  \n",
      "15    https://www.snopes.com/fact-check/shoe-stopper/  English  \n",
      "16  http://www.politifact.com/ohio/statements/2012...  English  \n",
      "17  https://www.snopes.com/fact-check/animas-river...  English  \n",
      "18  http://www.politifact.com/oregon/statements/20...  English  \n",
      "19  https://www.snopes.com/fact-check/missouri-rep...  English  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random as random\n",
    "import glob, os\n",
    "\n",
    "#concatener les fichiers en python  et lecture du fichier :\n",
    "df = pd.concat(map(pd.read_csv, glob.glob(os.path.join('', \"./DataSet/*.csv\"))))\n",
    "\n",
    "row, col = df.shape\n",
    "print(\"Size of the dataframe :\" + str(df.size) + \" (\"+str(row)+\"*\"+str(col)+\")\")\n",
    "\n",
    "\n",
    "# Afficher le nombre d'entrées NaN pour chaque colonne de notre dataframe\n",
    "print(\"\\n--------- Afficher le nombre d'entrées NaN pour chaque colonne de notre dataframe ---------\\n\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "#Afficher les différentes informations de notre dataset\n",
    "print(\"\\n--------- Afficher les différentes informations de notre dataset ---------\\n\")\n",
    "print(df.info())\n",
    "\n",
    "#Afficher les 20 premières entrées de notre dataset\n",
    "print(\"\\n--------- Afficher les 20 premières entrées de notre dataset ---------\\n\")\n",
    "print(df.head(20))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape 2 : Prétraitement du texte et traitement des valeurs manquantes\n",
    " On a fait les prétraitements suivants :\n",
    " - Transformation du texte en miniscule\n",
    " - Suppression des espaces\n",
    " - Enlever les ponctuations \n",
    " - Elimination des stopwords\n",
    " - Remplacement des mots de négation par le mot 'not'\n",
    " - Suppression des caractères non ASCII\n",
    " - Lemmatization \n",
    " - Correction orthographique \n",
    " - Conversion des nombres en mots\n",
    " - Remplacement des valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/depinfo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/depinfo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/depinfo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/depinfo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can only use .str accessor with string values!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c3e49f82bdc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# --- La mise en miniscule ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5173\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5174\u001b[0m         ):\n\u001b[0;32m-> 5175\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5176\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5177\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/accessor.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;31m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0maccessor_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;31m# Replace the property with the accessor object. Inspired by:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;31m# http://www.pydanny.com/cached-property.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/strings.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferred_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_categorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/strings.py\u001b[0m in \u001b[0;36m_validate\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minferred_dtype\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can only use .str accessor with string \"\u001b[0m \u001b[0;34m\"values!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .str accessor with string values!"
     ]
    }
   ],
   "source": [
    "#commencement des prétraitement :\n",
    "\n",
    "#import nécessaire :\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords') \n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import unicodedata\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "import inflect\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "\n",
    "# Fonctions crées:\n",
    "def nombreversmot(text): #Conversion des nombres vers mots\n",
    "    if text.isdigit():\n",
    "        return p.number_to_words(text)\n",
    "    else: return text\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def nltk2wn_tag(nltk_tag): #Lemmatization de tous type de mots\n",
    "  if nltk_tag.startswith('J'):\n",
    "    return wordnet.ADJ\n",
    "  elif nltk_tag.startswith('V'):\n",
    "    return wordnet.VERB\n",
    "  elif nltk_tag.startswith('N'):\n",
    "    return wordnet.NOUN\n",
    "  elif nltk_tag.startswith('R'):\n",
    "    return wordnet.ADV\n",
    "  else:                    \n",
    "    return None\n",
    "def lemmatize_sentence(sentence):\n",
    "  nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))    \n",
    "  wn_tagged = map(lambda x: (x[0], nltk2wn_tag(x[1])), nltk_tagged)\n",
    "  res_words = []\n",
    "  for word, tag in wn_tagged:\n",
    "    if tag is None:                        \n",
    "      res_words.append(word)\n",
    "    else:\n",
    "      res_words.append(lemmatizer.lemmatize(word, tag))\n",
    "  return \" \".join(res_words)\n",
    "\n",
    "# ------------------------------ Prétraitements sur la colonne text ----------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# --- La mise en miniscule ---\n",
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "\n",
    "# --- White spaces removal --- \n",
    "df['text'] = df['text'].str.strip()\n",
    "\n",
    "# --- Elimination des stopWord  --- \n",
    "\n",
    "stop_words = set(stopwords.words(\"english\")) #La liste des stopwords en Anglais\n",
    "negation ={'haven''t','cannot',\"doesn't\",\"shouldn't\",\"needn't\",\"shant't\",\"weren't\",\"hasn't\", \"wasn't\",\"didn't\", \"aren't\",'not', \"mightn't\", \"mustn't\", 'no',  \"wouldn't\", \"mightn't\", \"won't\",  \"needn't\", \"wasn't\", \"wouldn't\",  \"isn't\", \"doesn't\", \"weren't\", \"isn't\", \"hasn't\", \"hadn't\", \"don't\", \"hadn't\",\"couldn't\"} #La liste des stopwords de négation en Anglais  \n",
    "a= df['text'].str.replace(\"’\",\"'\") #Extraire toutes les entrées de la colonne text\n",
    "pat = r'\\b(?:{})\\b'.format('|'.join(negation))\n",
    "a=a.str.replace(pat,'not')\n",
    "pat='\\w*\\d\\w*'\n",
    "text_without_stopwords=[] #Une liste dont on va affecter les textes après l'élimination des stop words\n",
    "tk=TweetTokenizer()\n",
    "for text in a.iteritems(): #On parcourt toutes les lignes\n",
    " tokens = tk.tokenize(str(text[1]))\n",
    " result = [i for i in tokens if not i in stop_words-negation]\n",
    " splitor=\" \"\n",
    " concatinated = splitor.join(result) #concatiner les tokens\n",
    " text_without_stopwords.append(concatinated)\n",
    "\n",
    "df['text']=text_without_stopwords\n",
    "\n",
    "# --- Suppression des ponctuations  [!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]  --- \n",
    "\n",
    "text_without_punctuation=[] # pour y mettre notre résultat\n",
    "a= df['text'] #Extraire toutes les entrées de la colonne text deja traiter pour faire la suite\n",
    "tk=TweetTokenizer()\n",
    "for text in a.iteritems(): #On parcourt toutes les lignes\n",
    "    result = re.sub('[!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]', '', str(text[1]) )\n",
    "    splitor=\"\" #séparateur de mots\n",
    "    concatinated = splitor.join(result) #concatiner les résultats\n",
    "    text_without_punctuation.append(concatinated)\n",
    "\n",
    "df['text']=text_without_punctuation\n",
    "\n",
    "#  --- Suppression des caractères non ASCII  --- \n",
    "text_without_ascii=[] # pour y mettre notre résultat\n",
    "a= df['text'] #Extraire toutes les entrées de la colonne text deja traiter pour faire la suite\n",
    "tk=TweetTokenizer()\n",
    "for text in a.iteritems(): #On parcourt toutes les lignes\n",
    "    text = unicodedata.normalize('NFKD', str(text[1]) ).encode(\"ascii\", \"ignore\").decode(\"utf-8\", 'ignore')\n",
    "    splitor=\"\" #séparateur de mots\n",
    "    concatinated = splitor.join(text)\n",
    "    text_without_ascii.append(concatinated)\n",
    "\n",
    "df['text']=text_without_ascii\n",
    "\n",
    "#  --- Conversion des nombres en mots   --- \n",
    "print(\"---------- Avant Transformation des numériques en mots -------------------\")\n",
    "numbertransf=[] # pour y mettre notre résultat\n",
    "p = inflect.engine()\n",
    "a= df['text'] #Extraire toutes les entrées de la colonne text deja traiter pour faire la suite\n",
    "tk=TweetTokenizer()\n",
    "for text in a.iteritems():\n",
    "     tokens = tk.tokenize(str(text[1]))\n",
    "     result = [nombreversmot(i) for i in tokens]\n",
    "     splitor=\" \"\n",
    "     concatinated = splitor.join(result) #concatiner les tokens\n",
    "     numbertransf.append(concatinated)\n",
    "df['text']=numbertransf\n",
    "df['text'] = df['text'].str.strip()\n",
    "\n",
    "#  --- Lemmatization --- \n",
    "text_lemmatizer=[] # pour y mettre notre résultat\n",
    "a= df['text'] #Extraire toutes les entrées de la colonne text deja traiter pour faire la suite\n",
    "tk=TweetTokenizer()\n",
    "for ligne in a.iteritems(): #On parcourt toutes les lignes\n",
    "    newList=[]\n",
    "    newList.append(lemmatize_sentence(ligne[1])) #lemmatisation\n",
    "    splitor=\" \" #séparateur de mots\n",
    "    concatinated = splitor.join(newList)\n",
    "    text_lemmatizer.append(concatinated)\n",
    "df['text']=text_lemmatizer\n",
    "\n",
    "# #suppression des common word: (ANNULEE CAR CA PRENDS BEAUCOUP DE TEMPS ET CA CHANGE PAS BEACOUP DE CHOSES)\n",
    "# word_counter  = Counter()\n",
    "# for sentence in df[\"text\"].values:\n",
    "#     for word in sentence.split():\n",
    "#         word_counter[word] += 1\n",
    "# most = word_counter.most_common(10)\n",
    "# print(\"most common word\"+str(most))\n",
    "# print(\"Suppression of the common word de nos artiles : \")\n",
    "# most_word = set([w for (w, wc) in most])\n",
    "# def delmost_word(sentence):\n",
    "#     return \" \".join([word for word in str(sentence).split() if word not in most_word])\n",
    "# df[\"text\"] = df[\"text\"].apply(delmost_word)\n",
    "# df[\"text\"].head()\n",
    "\n",
    "# ------------------------------ FIN prétraitements sur la colonne text ----------------------------------------\n",
    "\n",
    "# ------------------------------ Prétraitements sur la colonne named_entities_claim ----------------------------\n",
    "\n",
    "# --- La mise en miniscule ---\n",
    "df['named_entities_claim']=df['named_entities_claim'].str.strip()\n",
    "\n",
    "# --- White spaces removal --- \n",
    "df['named_entities_claim']=df['named_entities_claim'].str.lower()\n",
    "\n",
    "# --- Le remplacement des valeurs manquantes par 'nan' ---\n",
    "df['named_entities_claim'] = df['named_entities_claim'].fillna('nan')\n",
    "\n",
    "# --- Elimination des stopWord  --- \n",
    "a= df['named_entities_claim'].str.replace(\"’\",\"'\")\n",
    "text_without_stopwords=[]\n",
    "tk=TweetTokenizer()\n",
    "for text in a.iteritems(): \n",
    " tokens = tk.tokenize(str(text[1]))\n",
    " result = [i for i in tokens if not i in stop_words-negation]\n",
    " splitor=\" \"\n",
    " concatinated = splitor.join(result)\n",
    " text_without_stopwords.append(concatinated)\n",
    "df['named_entities_claim']=text_without_stopwords\n",
    "\n",
    "#  --- Conversion des nombres en mots   --- \n",
    "numbertransf=[]\n",
    "p = inflect.engine()\n",
    "a= df['named_entities_claim'] \n",
    "tk=TweetTokenizer()\n",
    "for text in a.iteritems():\n",
    "     tokens = tk.tokenize(str(text[1]))\n",
    "     result = [nombreversmot(i) for i in tokens]\n",
    "     splitor=\" \"\n",
    "     concatinated = splitor.join(result) #concatiner les tokens\n",
    "     numbertransf.append(concatinated)\n",
    "df['named_entities_claim']=numbertransf\n",
    "df['named_entities_claim'] = df['named_entities_claim'].str.strip()\n",
    "\n",
    "#  --- Lemmatization --- \n",
    "text_lemmatizer=[] # pour y mettre notre résultat\n",
    "a= df['named_entities_claim'] #Extraire toutes les entrées de la colonne text deja traiter pour faire la suite\n",
    "tk=TweetTokenizer()\n",
    "for ligne in a.iteritems(): #On parcourt toutes les lignes\n",
    "    newList=[]\n",
    "    newList.append(lemmatize_sentence(ligne[1])) #lemmatisation\n",
    "    splitor=\" \" #séparateur de mots\n",
    "    concatinated = splitor.join(newList)\n",
    "    text_lemmatizer.append(concatinated)\n",
    "df['named_entities_claim']=text_lemmatizer\n",
    "\n",
    "# ------------------------------ FIN prétraitements sur la colonne named_entities_claim -----------------------\n",
    "\n",
    "# ------------------------------ Prétraitements sur la colonne keyword ----------------------------------------\n",
    "\n",
    "\n",
    "df['keywords'] = df['keywords'].str.lower()\n",
    "\n",
    "\n",
    "#White spaces removal\n",
    "df['keywords'] = df['keywords'].str.strip()\n",
    "df['keywords'] = df['keywords'].fillna('nan')\n",
    "\n",
    "\n",
    "# ------------------------------ FIN prétraitements sur la colonne keyword ------------------------------------\n",
    "\n",
    "# df['headline']=df['headline'].str.strip()\n",
    "# df['headline']=df['headline'].str.lower()\n",
    "# df['headline'] = df['headline'].fillna('nan')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #Lemmatization :\n",
    "# text_lemmatizer=[] # pour y mettre notre résultat\n",
    "# a= df['headline'] #Extraire toutes les entrées de la colonne text deja traiter pour faire la suite\n",
    "# tk=TweetTokenizer()\n",
    "# for ligne in a.iteritems(): #On parcourt toutes les lignes\n",
    "#     newList=[]\n",
    "#     newList.append(lemmatize_sentence(ligne[1])) #lemmatisation\n",
    "#     splitor=\" \" #séparateur de mots\n",
    "#     concatinated = splitor.join(newList)\n",
    "#     text_lemmatizer.append(concatinated)\n",
    "# df['headline']=text_lemmatizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df\n",
    "\n",
    "#Fin des prétraitement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape 3 : Conversion des textes en valeurs numériques\n",
    " On transforme nos données de textes en valeurs numériques, en utilisant des LabelEncoder sur les colonnes qu'on a  sauf la colonne texte et la colonne keyword, dont on a essayé Tf-IDF pour les transformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       and  say  thousand  two\n",
      "0        0    0         0    0\n",
      "1        0    0         0    0\n",
      "2        0    1         0    0\n",
      "3        0    1         0    0\n",
      "4        0    1         0    0\n",
      "...    ...  ...       ...  ...\n",
      "14447    0    0         0    0\n",
      "14448    1    0         1    1\n",
      "14449    0    0         0    0\n",
      "14450    1    0         1    1\n",
      "14451    0    0         0    0\n",
      "\n",
      "[14452 rows x 4 columns]\n",
      "       barack  donald  facebook  house  http  internet  john  national  new  \\\n",
      "0           0       0         0      0     0         0     0         0    0   \n",
      "1           0       0         0      0     0         0     0         0    0   \n",
      "2           0       0         0      0     0         0     0         0    0   \n",
      "3           0       0         0      0     0         0     0         0    0   \n",
      "4           0       0         0      0     0         0     0         0    0   \n",
      "...       ...     ...       ...    ...   ...       ...   ...       ...  ...   \n",
      "14447       0       0         0      0     0         0     0         0    0   \n",
      "14448       0       0         0      0     0         0     0         0    1   \n",
      "14449       0       0         0      0     0         0     0         0    0   \n",
      "14450       0       0         0      0     0         0     1         0    0   \n",
      "14451       0       0         0      0     0         0     0         0    0   \n",
      "\n",
      "       news  ...  politifact  republican  state  time  trump  twitter  \\\n",
      "0         0  ...           0           0      0     0      0        0   \n",
      "1         0  ...           0           0      0     0      0        0   \n",
      "2         0  ...           0           0      0     0      0        0   \n",
      "3         0  ...           0           0      0     0      0        0   \n",
      "4         0  ...           0           0      0     0      0        0   \n",
      "...     ...  ...         ...         ...    ...   ...    ...      ...   \n",
      "14447     0  ...           0           0      0     0      0        0   \n",
      "14448     0  ...           0           0      0     0      0        0   \n",
      "14449     0  ...           0           0      0     0      0        0   \n",
      "14450     0  ...           0           0      0     0      0        0   \n",
      "14451     0  ...           0           0      2     0      0        0   \n",
      "\n",
      "       university  washington  white  york  \n",
      "0               0           0      0     0  \n",
      "1               0           0      0     0  \n",
      "2               0           0      0     0  \n",
      "3               0           0      0     0  \n",
      "4               0           0      0     0  \n",
      "...           ...         ...    ...   ...  \n",
      "14447           0           0      0     0  \n",
      "14448           0           0      0     0  \n",
      "14449           0           0      0     0  \n",
      "14450           0           0      0     0  \n",
      "14451           0           0      0     0  \n",
      "\n",
      "[14452 rows x 21 columns]\n",
      "\n",
      "Ajout des colonne au dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/depinfo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:66: FutureWarning: The join_axes-keyword is deprecated. Use .reindex or .reindex_like on the result to achieve the same functionality.\n",
      "/home/depinfo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:67: FutureWarning: The join_axes-keyword is deprecated. Use .reindex or .reindex_like on the result to achieve the same functionality.\n",
      "/home/depinfo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:68: FutureWarning: The join_axes-keyword is deprecated. Use .reindex or .reindex_like on the result to achieve the same functionality.\n",
      "/home/depinfo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:69: FutureWarning: The join_axes-keyword is deprecated. Use .reindex or .reindex_like on the result to achieve the same functionality.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#np.random.seed(500) #utilisé pour avoir le même résultat à chaque exécution \n",
    "#On crée des variable LabelEncoder qui vont servir à transférer nos données en valeurs numériques\n",
    "l1=LabelEncoder()\n",
    "l2=LabelEncoder()\n",
    "l3=LabelEncoder()\n",
    "l4=LabelEncoder()\n",
    "l5=LabelEncoder()\n",
    "l6=LabelEncoder()\n",
    "l7=LabelEncoder()\n",
    "l8=LabelEncoder()\n",
    "l9=LabelEncoder()\n",
    "l10=LabelEncoder()\n",
    "l11=LabelEncoder()\n",
    "l12=LabelEncoder()\n",
    "l13=LabelEncoder()\n",
    "df=df.applymap(str) #on transforme tous nous données en String car y'avais des entrées qui ont une combinaison du string et float\n",
    "#On applique la mesure TF-IDF sur la colonne text et la colonne keywords\n",
    "# Tfidf_vect = TfidfVectorizer(max_features=40000,min_df=0.03, max_df=0.7)\n",
    "# tfidfx=Tfidf_vect.fit_transform(df['named_entities_claim'])\n",
    "# df1 = pd.DataFrame(tfidfx.toarray(), columns=Tfidf_vect.get_feature_names())\n",
    "# Tfidf_vect = TfidfVectorizer(max_features=4000)\n",
    "# print(df['named_entities_claim'])\n",
    "cd = CountVectorizer(min_df=0.1)\n",
    "cd.fit(df[\"text\"])\n",
    "tfidfx=cd.transform(df['text'])\n",
    "df2 = pd.DataFrame(tfidfx.toarray(), columns=cd.get_feature_names())\n",
    "print(df2)\n",
    "cd = CountVectorizer(min_df=0.1)\n",
    "cd.fit(df[\"named_entities_claim\"])\n",
    "tfidfx=cd.transform(df['text'])\n",
    "df1 = pd.DataFrame(tfidfx.toarray(), columns=cd.get_feature_names())\n",
    "print(df1)\n",
    "\n",
    "# cd = CountVectorizer(min_df=0.1)\n",
    "# cd.fit(df[\"headline\"])\n",
    "# tfidfx=cd.transform(df['text'])\n",
    "# df3 = pd.DataFrame(tfidfx.toarray(), columns=cd.get_feature_names())\n",
    "# print(df3)\n",
    "\n",
    "#on transfère toutes les valeurs des colonnes qu'on va utiliser en valeurs numériques \n",
    "df['id']=l1.fit_transform(df['id'])\n",
    "df['text']=l3.fit_transform(df['text'])\n",
    "df['date']=l3.fit_transform(df['date'])\n",
    "df['author']=l5.fit_transform(df['author'])\n",
    "df['headline']=l6.fit_transform(df['headline'])\n",
    "# df['named_entities_claim']=l7.fit_transform(df['named_entities_claim'])\n",
    "df['named_entities_article']=l8.fit_transform(df['named_entities_article'])\n",
    "df['sourceURL']=l11.fit_transform(df['sourceURL'])\n",
    "df['link']=l12.fit_transform(df['link'])\n",
    "df['language']=l13.fit_transform(df['language'])\n",
    "df['ratingName']=l13.fit_transform(df['ratingName'])\n",
    "print (\"\\nAjout des colonne au dataframe\")\n",
    "dummies = pd.get_dummies(df['source'],prefix ='Src')\n",
    "\n",
    "\n",
    "\n",
    "df= pd.concat([df, df1], axis=1,join_axes=[df.index])\n",
    "df=pd.concat([df, df2], axis=1,join_axes=[df.index])\n",
    "df= pd.concat([df, dummies], axis=1,join_axes=[df.index])\n",
    "df1=pd.concat([df1, df2], axis=1,join_axes=[df1.index])\n",
    "# df=pd.concat([df, df3], axis=1,join_axes=[df.index])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape 4 : Dévision du jeu de données en données d'entraînement et données de test\n",
    "On mélange le dataframe qu'on a et après on sélectionne 80% des données pour l'entraînement et 20% pour le test, ensuite on sélectionne les colonnes features dont on s'intéresse lors du classification, et on essaye plusieurs combinaisons de colonnes pour arriver à une meilleure accuracy (#TODO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Src_africacheck', 'Src_factscan', 'Src_politifact', 'Src_snopes', 'Src_truthorfiction', 'barack', 'donald', 'facebook', 'house', 'http', 'internet', 'john', 'national', 'new', 'news', 'obama', 'politifact', 'republican', 'state', 'time', 'trump', 'twitter', 'university', 'washington', 'white', 'york', 'and', 'say', 'thousand', 'two']\n",
      "Accuracy: 0.32099619508820476\n",
      "Accuracy: 0.7021791767554479\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "##### \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "X_train, X_test ,y_train, y_test = train_test_split(df,df[\"truthRating\"], test_size=0.2, random_state=int(time.time()))\n",
    "\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "used_features =['Src_africacheck', 'Src_factscan', 'Src_politifact', 'Src_snopes', 'Src_truthorfiction']\n",
    "\n",
    "for col in df1.columns :\n",
    "    used_features.append(col)\n",
    "\n",
    "\n",
    "print(used_features)\n",
    "gnb.fit(\n",
    "    X_train[used_features].values,\n",
    "    y_train\n",
    ")\n",
    "mnb=MultinomialNB()\n",
    "mnb.fit(X_train[used_features].values,\n",
    "    y_train)\n",
    "\n",
    "y_pred = gnb.predict(X_test[used_features])\n",
    "mnbpred=mnb.predict(X_test[used_features])\n",
    "# print(\"Number of mislabeled points out of a total {} points : {}, performance {:05.2f}%\"\n",
    "#       .format(\n",
    "#           X_test.shape[0],\n",
    "#           (X_test[\"ratingName\"] != y_pred).sum(),\n",
    "#           100*(1-(X_test[\"ratingName\"] != y_pred).sum()/X_test.shape[0])\n",
    "# ))\n",
    "# print(df['ratingName'])\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, mnbpred))\n",
    "print(len(used_features))##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape 5 :  Entraînement et prédiction en utilisant des modèles de \n",
    "\n",
    "\n",
    "classification\n",
    "On passe les features et targets des données d'entraînement à nos classifiers (#TODO tester plusieurs classifieurs) et après on lui laisse prédire les valeurs des données de test (soit vrai, faux ou mixture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin by importing all necessary libraries\n",
    "# import pandas as pd\n",
    "# import time\n",
    "# from sklearn.utils import shuffle\n",
    "# import numpy as np\n",
    "# ##### \n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "\n",
    "# #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=int(time.time()))\n",
    "# label = df[\"ratingName\"]\n",
    "# features=zip(df[\"date\"],df[\"author\"],df[\"named_entities_claim\"],df[\"source\"],df[\"sourceURL\"])\n",
    "# # dt = df[\"date\"]\n",
    "# # src = df[\"sourceURL\"]\n",
    "# # features=zip(dt,src)\n",
    "\n",
    "# pip\n",
    "\n",
    "# #print(df[\"author\"])  \n",
    "# print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
