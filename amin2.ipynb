{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Fact-checking project using pandas and ScikitLearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travail réalisé par : Bouali Mohammed-Amin, Oussama Nassim Sehout, Chahinez Benallal, Abdellah Choukri \n",
    "\n",
    "\n",
    "\n",
    "####  Sommaire du travail :\n",
    "####   . Chargement du jeu de données\n",
    "####   . Prétraitement du texte \n",
    "###   . Conversion des textes aux valeurs numériques\n",
    "###   . Division du jeu de données en données d'entraînement et données de test\n",
    "###   . Entraînement et prédiction en utilisant des modèles de classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 1 : Chargement du jeu de données  \n",
    " On a généré trois fichiers csv, un avec valeurs true, un autre avec valeurs false, et un dernier avec valeurs mixture à partir du site ClaimsKg, après on les concatène pour avoir notre jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataframe :202328 (14452*14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>truthRating</th>\n",
       "      <th>ratingName</th>\n",
       "      <th>author</th>\n",
       "      <th>headline</th>\n",
       "      <th>named_entities_claim</th>\n",
       "      <th>named_entities_article</th>\n",
       "      <th>keywords</th>\n",
       "      <th>source</th>\n",
       "      <th>sourceURL</th>\n",
       "      <th>link</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/36...</td>\n",
       "      <td>'There will be no public funding for abortion ...</td>\n",
       "      <td>2010-03-21</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>Bart Stupak</td>\n",
       "      <td>Stupak revises abortion stance on health care ...</td>\n",
       "      <td>Abortion rights,Barack Obama,Bart Stupak,Ben N...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>Abortion,Health Care</td>\n",
       "      <td>politifact</td>\n",
       "      <td>http://www.politifact.com</td>\n",
       "      <td>http://www.politifact.com/truth-o-meter/statem...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/e6...</td>\n",
       "      <td>Central Health 'is the only hospital district ...</td>\n",
       "      <td>2011-03-15</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>Wayne Christian</td>\n",
       "      <td>State Rep. Wayne Christian says Central Health...</td>\n",
       "      <td>Austin American-Statesman,Harris County Hospit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abortion</td>\n",
       "      <td>politifact</td>\n",
       "      <td>http://www.politifact.com</td>\n",
       "      <td>http://www.politifact.com/texas/statements/201...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/e0...</td>\n",
       "      <td>Says most of Perry's chiefs of staff have been...</td>\n",
       "      <td>2010-08-14</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>Bill White</td>\n",
       "      <td>Bill White says most of Gov. Rick Perry's chie...</td>\n",
       "      <td>AT&amp;T,Bill Clements,Bill White,Bracewell &amp; Giul...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ethics</td>\n",
       "      <td>politifact</td>\n",
       "      <td>http://www.politifact.com</td>\n",
       "      <td>http://www.politifact.com/texas/statements/201...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/48...</td>\n",
       "      <td>Says 'as Co-Chair of the Joint Ways &amp; Means Co...</td>\n",
       "      <td>2012-09-28</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>Mary Nolan</td>\n",
       "      <td>Did Mary Nolan secure funding for Milwaukie br...</td>\n",
       "      <td>Carolyn Tomei,Dave Hunt,Fetsch,Jeff Merkley,Ka...</td>\n",
       "      <td>Portland-Milwaukie Light Rail project</td>\n",
       "      <td>State Budget,State Finances,Transportation</td>\n",
       "      <td>politifact</td>\n",
       "      <td>http://www.politifact.com</td>\n",
       "      <td>http://www.politifact.com/oregon/statements/20...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/80...</td>\n",
       "      <td>Says Gary Farmer’s claim that he 'received an ...</td>\n",
       "      <td>2016-07-08</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>Jim Waldman</td>\n",
       "      <td>Florida Senate candidate never actually receiv...</td>\n",
       "      <td>Gary Farmer,Gwyndolen Clarke-Reed,Jim Waldman,...</td>\n",
       "      <td>Gary Farmer</td>\n",
       "      <td>Guns</td>\n",
       "      <td>politifact</td>\n",
       "      <td>http://www.politifact.com</td>\n",
       "      <td>http://www.politifact.com/florida/statements/2...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9995</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/cd...</td>\n",
       "      <td>Kevin’s lovable yet geeky sidekick in TV’s “Th...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Marilyn Manson and The Wonder Years</td>\n",
       "      <td>Alice Cooper,Behind the Green Door,Bewitched,B...</td>\n",
       "      <td>Marilyn Manson,Wonder Years</td>\n",
       "      <td>Artists, ASP Article, music, Radio &amp; TV, Telev...</td>\n",
       "      <td>snopes</td>\n",
       "      <td>http://www.snopes.com</td>\n",
       "      <td>https://www.snopes.com/fact-check/wondering-ab...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9996</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/7f...</td>\n",
       "      <td>E-mail reproduces George Carlin’s list of New ...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>George Carlin New Rules for 2006</td>\n",
       "      <td>George Carlin,HBO,Real Time with Bill Maher,th...</td>\n",
       "      <td>George Carlin</td>\n",
       "      <td>ASP Article</td>\n",
       "      <td>snopes</td>\n",
       "      <td>http://www.snopes.com</td>\n",
       "      <td>https://www.snopes.com/fact-check/new-rules-fo...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9997</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/72...</td>\n",
       "      <td>'Obamacare Medical Codes Confirm: Execution by...</td>\n",
       "      <td>2013-11-23</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Bloggers</td>\n",
       "      <td>Bloggers say Obamacare coding system could ush...</td>\n",
       "      <td>American Medical Association,Centers for Disea...</td>\n",
       "      <td>Medical Codes</td>\n",
       "      <td>Health Care,Legal Issues,Public Health</td>\n",
       "      <td>politifact</td>\n",
       "      <td>http://www.politifact.com</td>\n",
       "      <td>http://www.politifact.com/truth-o-meter/statem...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9998</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/7c...</td>\n",
       "      <td>Wrestler John Cena died in a car accident in J...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>John Cena Death Hoax</td>\n",
       "      <td>Facebook,Interstate 80,John Cena,WWE,fake news...</td>\n",
       "      <td>John Cena</td>\n",
       "      <td>death hoax, john cena, john cena dead</td>\n",
       "      <td>snopes</td>\n",
       "      <td>http://www.snopes.com</td>\n",
       "      <td>https://www.snopes.com/fact-check/john-cena-de...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9999</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/2d...</td>\n",
       "      <td>Missouri is one of '13 states this year have s...</td>\n",
       "      <td>2017-11-29</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Trump misses mark on Missouri unemployment rate</td>\n",
       "      <td>Bureau of Labor Statistics,Donald Trump,White ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Economy,Jobs</td>\n",
       "      <td>politifact</td>\n",
       "      <td>http://www.politifact.com</td>\n",
       "      <td>http://www.politifact.com/missouri/statements/...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14452 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     id  \\\n",
       "0     http://data.gesis.org/claimskg/claim_review/36...   \n",
       "1     http://data.gesis.org/claimskg/claim_review/e6...   \n",
       "2     http://data.gesis.org/claimskg/claim_review/e0...   \n",
       "3     http://data.gesis.org/claimskg/claim_review/48...   \n",
       "4     http://data.gesis.org/claimskg/claim_review/80...   \n",
       "...                                                 ...   \n",
       "9995  http://data.gesis.org/claimskg/claim_review/cd...   \n",
       "9996  http://data.gesis.org/claimskg/claim_review/7f...   \n",
       "9997  http://data.gesis.org/claimskg/claim_review/72...   \n",
       "9998  http://data.gesis.org/claimskg/claim_review/7c...   \n",
       "9999  http://data.gesis.org/claimskg/claim_review/2d...   \n",
       "\n",
       "                                                   text        date  \\\n",
       "0     'There will be no public funding for abortion ...  2010-03-21   \n",
       "1     Central Health 'is the only hospital district ...  2011-03-15   \n",
       "2     Says most of Perry's chiefs of staff have been...  2010-08-14   \n",
       "3     Says 'as Co-Chair of the Joint Ways & Means Co...  2012-09-28   \n",
       "4     Says Gary Farmer’s claim that he 'received an ...  2016-07-08   \n",
       "...                                                 ...         ...   \n",
       "9995  Kevin’s lovable yet geeky sidekick in TV’s “Th...     Unknown   \n",
       "9996  E-mail reproduces George Carlin’s list of New ...     Unknown   \n",
       "9997  'Obamacare Medical Codes Confirm: Execution by...  2013-11-23   \n",
       "9998  Wrestler John Cena died in a car accident in J...     Unknown   \n",
       "9999  Missouri is one of '13 states this year have s...  2017-11-29   \n",
       "\n",
       "      truthRating  ratingName           author  \\\n",
       "0               3        True      Bart Stupak   \n",
       "1               3        True  Wayne Christian   \n",
       "2               3        True       Bill White   \n",
       "3               3        True       Mary Nolan   \n",
       "4               3        True      Jim Waldman   \n",
       "...           ...         ...              ...   \n",
       "9995            1       False          Unknown   \n",
       "9996            1       False          Unknown   \n",
       "9997            1       False         Bloggers   \n",
       "9998            1       False          Unknown   \n",
       "9999            1       False     Donald Trump   \n",
       "\n",
       "                                               headline  \\\n",
       "0     Stupak revises abortion stance on health care ...   \n",
       "1     State Rep. Wayne Christian says Central Health...   \n",
       "2     Bill White says most of Gov. Rick Perry's chie...   \n",
       "3     Did Mary Nolan secure funding for Milwaukie br...   \n",
       "4     Florida Senate candidate never actually receiv...   \n",
       "...                                                 ...   \n",
       "9995                Marilyn Manson and The Wonder Years   \n",
       "9996                   George Carlin New Rules for 2006   \n",
       "9997  Bloggers say Obamacare coding system could ush...   \n",
       "9998                               John Cena Death Hoax   \n",
       "9999    Trump misses mark on Missouri unemployment rate   \n",
       "\n",
       "                                   named_entities_claim  \\\n",
       "0     Abortion rights,Barack Obama,Bart Stupak,Ben N...   \n",
       "1     Austin American-Statesman,Harris County Hospit...   \n",
       "2     AT&T,Bill Clements,Bill White,Bracewell & Giul...   \n",
       "3     Carolyn Tomei,Dave Hunt,Fetsch,Jeff Merkley,Ka...   \n",
       "4     Gary Farmer,Gwyndolen Clarke-Reed,Jim Waldman,...   \n",
       "...                                                 ...   \n",
       "9995  Alice Cooper,Behind the Green Door,Bewitched,B...   \n",
       "9996  George Carlin,HBO,Real Time with Bill Maher,th...   \n",
       "9997  American Medical Association,Centers for Disea...   \n",
       "9998  Facebook,Interstate 80,John Cena,WWE,fake news...   \n",
       "9999  Bureau of Labor Statistics,Donald Trump,White ...   \n",
       "\n",
       "                     named_entities_article  \\\n",
       "0                                  abortion   \n",
       "1                                       NaN   \n",
       "2                                       NaN   \n",
       "3     Portland-Milwaukie Light Rail project   \n",
       "4                               Gary Farmer   \n",
       "...                                     ...   \n",
       "9995            Marilyn Manson,Wonder Years   \n",
       "9996                          George Carlin   \n",
       "9997                          Medical Codes   \n",
       "9998                              John Cena   \n",
       "9999                                    NaN   \n",
       "\n",
       "                                               keywords      source  \\\n",
       "0                                  Abortion,Health Care  politifact   \n",
       "1                                              Abortion  politifact   \n",
       "2                                                Ethics  politifact   \n",
       "3            State Budget,State Finances,Transportation  politifact   \n",
       "4                                                  Guns  politifact   \n",
       "...                                                 ...         ...   \n",
       "9995  Artists, ASP Article, music, Radio & TV, Telev...      snopes   \n",
       "9996                                        ASP Article      snopes   \n",
       "9997             Health Care,Legal Issues,Public Health  politifact   \n",
       "9998              death hoax, john cena, john cena dead      snopes   \n",
       "9999                                       Economy,Jobs  politifact   \n",
       "\n",
       "                      sourceURL  \\\n",
       "0     http://www.politifact.com   \n",
       "1     http://www.politifact.com   \n",
       "2     http://www.politifact.com   \n",
       "3     http://www.politifact.com   \n",
       "4     http://www.politifact.com   \n",
       "...                         ...   \n",
       "9995      http://www.snopes.com   \n",
       "9996      http://www.snopes.com   \n",
       "9997  http://www.politifact.com   \n",
       "9998      http://www.snopes.com   \n",
       "9999  http://www.politifact.com   \n",
       "\n",
       "                                                   link language  \n",
       "0     http://www.politifact.com/truth-o-meter/statem...  English  \n",
       "1     http://www.politifact.com/texas/statements/201...  English  \n",
       "2     http://www.politifact.com/texas/statements/201...  English  \n",
       "3     http://www.politifact.com/oregon/statements/20...  English  \n",
       "4     http://www.politifact.com/florida/statements/2...  English  \n",
       "...                                                 ...      ...  \n",
       "9995  https://www.snopes.com/fact-check/wondering-ab...  English  \n",
       "9996  https://www.snopes.com/fact-check/new-rules-fo...  English  \n",
       "9997  http://www.politifact.com/truth-o-meter/statem...  English  \n",
       "9998  https://www.snopes.com/fact-check/john-cena-de...  English  \n",
       "9999  http://www.politifact.com/missouri/statements/...  English  \n",
       "\n",
       "[14452 rows x 14 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random as random\n",
    "import glob, os\n",
    "\n",
    "#concatener les fichiers en python  et lecture du fichier :\n",
    "df = pd.concat(map(pd.read_csv, glob.glob(os.path.join('', \"./DataSet/*.csv\"))))\n",
    "\n",
    "row, col = df.shape\n",
    "print(\"Size of the dataframe :\" + str(df.size) + \" (\"+str(row)+\"*\"+str(col)+\")\")\n",
    "\n",
    "df.head()\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape 2 : Prétraitement du texte\n",
    " On a fait les prétraitements suivants :\n",
    "     . Transformation du texte en miniscule\n",
    "     . Suppression des espaces\n",
    "     . Enlever les ponctuations \n",
    "     . Elimination des stopwords\n",
    "     . Remplacement des mots de négation par le mot 'not'\n",
    "     . Suppression des caractères non ASCII\n",
    "     . Lemmatization \n",
    "     . Correction orthographique ##TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/depinfo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/depinfo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/depinfo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/depinfo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Avant Transformation des numériques en mots -------------------\n",
      "0    abortion rights , barack obama , bart stupak ,...\n",
      "1    austin american-statesman , harris county hosp...\n",
      "2    & , bill clements , bill white , bracewell & g...\n",
      "3    carolyn tomei , dave hunt , fetsch , jeff merk...\n",
      "4    gary farmer , gwyndolen clarke-reed , jim wald...\n",
      "5    democratic governors association , john chafee...\n",
      "6    114th congress , two thousand and sixteen gene...\n",
      "7    two thousand and sixteen presidential campaign...\n",
      "8    cuba , facebook , guantanamo bay , guantánamo ...\n",
      "9    afghanistan , christmas decoration , japan , l...\n",
      "Name: named_entities_claim, dtype: object\n",
      "---------- Après Transformation des numériques en mots -------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>truthRating</th>\n",
       "      <th>ratingName</th>\n",
       "      <th>author</th>\n",
       "      <th>headline</th>\n",
       "      <th>named_entities_claim</th>\n",
       "      <th>named_entities_article</th>\n",
       "      <th>keywords</th>\n",
       "      <th>source</th>\n",
       "      <th>sourceURL</th>\n",
       "      <th>link</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/36...</td>\n",
       "      <td>' not public funding abortion legislation '</td>\n",
       "      <td>2010-03-21</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>Bart Stupak</td>\n",
       "      <td>Stupak revises abortion stance on health care ...</td>\n",
       "      <td>abortion right , barack obama , bart stupak , ...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>abortion,health care</td>\n",
       "      <td>politifact</td>\n",
       "      <td>http://www.politifact.com</td>\n",
       "      <td>http://www.politifact.com/truth-o-meter/statem...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/e6...</td>\n",
       "      <td>central health ' hospital district texas spend...</td>\n",
       "      <td>2011-03-15</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>Wayne Christian</td>\n",
       "      <td>State Rep. Wayne Christian says Central Health...</td>\n",
       "      <td>austin american-statesman , harris county hosp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abortion</td>\n",
       "      <td>politifact</td>\n",
       "      <td>http://www.politifact.com</td>\n",
       "      <td>http://www.politifact.com/texas/statements/201...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/e0...</td>\n",
       "      <td>say perry 's chiefs staff lobbyist</td>\n",
       "      <td>2010-08-14</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>Bill White</td>\n",
       "      <td>Bill White says most of Gov. Rick Perry's chie...</td>\n",
       "      <td>&amp; , bill clements , bill white , bracewell &amp; g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ethics</td>\n",
       "      <td>politifact</td>\n",
       "      <td>http://www.politifact.com</td>\n",
       "      <td>http://www.politifact.com/texas/statements/201...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/48...</td>\n",
       "      <td>say ' cochair joint way mean committee secure ...</td>\n",
       "      <td>2012-09-28</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>Mary Nolan</td>\n",
       "      <td>Did Mary Nolan secure funding for Milwaukie br...</td>\n",
       "      <td>carolyn tomei , dave hunt , fetsch , jeff merk...</td>\n",
       "      <td>Portland-Milwaukie Light Rail project</td>\n",
       "      <td>state budget,state finances,transportation</td>\n",
       "      <td>politifact</td>\n",
       "      <td>http://www.politifact.com</td>\n",
       "      <td>http://www.politifact.com/oregon/statements/20...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/80...</td>\n",
       "      <td>say gary farmer 's claim ' receive ' ' nra ' '...</td>\n",
       "      <td>2016-07-08</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>Jim Waldman</td>\n",
       "      <td>Florida Senate candidate never actually receiv...</td>\n",
       "      <td>gary farmer , gwyndolen clarke-reed , jim wald...</td>\n",
       "      <td>Gary Farmer</td>\n",
       "      <td>guns</td>\n",
       "      <td>politifact</td>\n",
       "      <td>http://www.politifact.com</td>\n",
       "      <td>http://www.politifact.com/florida/statements/2...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9995</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/cd...</td>\n",
       "      <td>kevin 's lovable yet geeky sidekick tv 's wond...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Marilyn Manson and The Wonder Years</td>\n",
       "      <td>alice cooper , behind green door , bewitch , b...</td>\n",
       "      <td>Marilyn Manson,Wonder Years</td>\n",
       "      <td>artists, asp article, music, radio &amp; tv, telev...</td>\n",
       "      <td>snopes</td>\n",
       "      <td>http://www.snopes.com</td>\n",
       "      <td>https://www.snopes.com/fact-check/wondering-ab...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9996</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/7f...</td>\n",
       "      <td>email reproduces george carlin 's list new rul...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>George Carlin New Rules for 2006</td>\n",
       "      <td>george carlin , hbo , real time bill maher , i...</td>\n",
       "      <td>George Carlin</td>\n",
       "      <td>asp article</td>\n",
       "      <td>snopes</td>\n",
       "      <td>http://www.snopes.com</td>\n",
       "      <td>https://www.snopes.com/fact-check/new-rules-fo...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9997</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/72...</td>\n",
       "      <td>' obamacare medical code confirm execution beh...</td>\n",
       "      <td>2013-11-23</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Bloggers</td>\n",
       "      <td>Bloggers say Obamacare coding system could ush...</td>\n",
       "      <td>american medical association , center disease ...</td>\n",
       "      <td>Medical Codes</td>\n",
       "      <td>health care,legal issues,public health</td>\n",
       "      <td>politifact</td>\n",
       "      <td>http://www.politifact.com</td>\n",
       "      <td>http://www.politifact.com/truth-o-meter/statem...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9998</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/7c...</td>\n",
       "      <td>wrestler john cena die car accident july two t...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>John Cena Death Hoax</td>\n",
       "      <td>facebook , interstate eighty , john cena , wwe...</td>\n",
       "      <td>John Cena</td>\n",
       "      <td>death hoax, john cena, john cena dead</td>\n",
       "      <td>snopes</td>\n",
       "      <td>http://www.snopes.com</td>\n",
       "      <td>https://www.snopes.com/fact-check/john-cena-de...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9999</td>\n",
       "      <td>http://data.gesis.org/claimskg/claim_review/2d...</td>\n",
       "      <td>missouri one ' thirteen state year see unemplo...</td>\n",
       "      <td>2017-11-29</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Trump misses mark on Missouri unemployment rate</td>\n",
       "      <td>bureau labor statistic , donald trump , white ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>economy,jobs</td>\n",
       "      <td>politifact</td>\n",
       "      <td>http://www.politifact.com</td>\n",
       "      <td>http://www.politifact.com/missouri/statements/...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14452 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     id  \\\n",
       "0     http://data.gesis.org/claimskg/claim_review/36...   \n",
       "1     http://data.gesis.org/claimskg/claim_review/e6...   \n",
       "2     http://data.gesis.org/claimskg/claim_review/e0...   \n",
       "3     http://data.gesis.org/claimskg/claim_review/48...   \n",
       "4     http://data.gesis.org/claimskg/claim_review/80...   \n",
       "...                                                 ...   \n",
       "9995  http://data.gesis.org/claimskg/claim_review/cd...   \n",
       "9996  http://data.gesis.org/claimskg/claim_review/7f...   \n",
       "9997  http://data.gesis.org/claimskg/claim_review/72...   \n",
       "9998  http://data.gesis.org/claimskg/claim_review/7c...   \n",
       "9999  http://data.gesis.org/claimskg/claim_review/2d...   \n",
       "\n",
       "                                                   text        date  \\\n",
       "0           ' not public funding abortion legislation '  2010-03-21   \n",
       "1     central health ' hospital district texas spend...  2011-03-15   \n",
       "2                    say perry 's chiefs staff lobbyist  2010-08-14   \n",
       "3     say ' cochair joint way mean committee secure ...  2012-09-28   \n",
       "4     say gary farmer 's claim ' receive ' ' nra ' '...  2016-07-08   \n",
       "...                                                 ...         ...   \n",
       "9995  kevin 's lovable yet geeky sidekick tv 's wond...     Unknown   \n",
       "9996  email reproduces george carlin 's list new rul...     Unknown   \n",
       "9997  ' obamacare medical code confirm execution beh...  2013-11-23   \n",
       "9998  wrestler john cena die car accident july two t...     Unknown   \n",
       "9999  missouri one ' thirteen state year see unemplo...  2017-11-29   \n",
       "\n",
       "      truthRating  ratingName           author  \\\n",
       "0               3        True      Bart Stupak   \n",
       "1               3        True  Wayne Christian   \n",
       "2               3        True       Bill White   \n",
       "3               3        True       Mary Nolan   \n",
       "4               3        True      Jim Waldman   \n",
       "...           ...         ...              ...   \n",
       "9995            1       False          Unknown   \n",
       "9996            1       False          Unknown   \n",
       "9997            1       False         Bloggers   \n",
       "9998            1       False          Unknown   \n",
       "9999            1       False     Donald Trump   \n",
       "\n",
       "                                               headline  \\\n",
       "0     Stupak revises abortion stance on health care ...   \n",
       "1     State Rep. Wayne Christian says Central Health...   \n",
       "2     Bill White says most of Gov. Rick Perry's chie...   \n",
       "3     Did Mary Nolan secure funding for Milwaukie br...   \n",
       "4     Florida Senate candidate never actually receiv...   \n",
       "...                                                 ...   \n",
       "9995                Marilyn Manson and The Wonder Years   \n",
       "9996                   George Carlin New Rules for 2006   \n",
       "9997  Bloggers say Obamacare coding system could ush...   \n",
       "9998                               John Cena Death Hoax   \n",
       "9999    Trump misses mark on Missouri unemployment rate   \n",
       "\n",
       "                                   named_entities_claim  \\\n",
       "0     abortion right , barack obama , bart stupak , ...   \n",
       "1     austin american-statesman , harris county hosp...   \n",
       "2     & , bill clements , bill white , bracewell & g...   \n",
       "3     carolyn tomei , dave hunt , fetsch , jeff merk...   \n",
       "4     gary farmer , gwyndolen clarke-reed , jim wald...   \n",
       "...                                                 ...   \n",
       "9995  alice cooper , behind green door , bewitch , b...   \n",
       "9996  george carlin , hbo , real time bill maher , i...   \n",
       "9997  american medical association , center disease ...   \n",
       "9998  facebook , interstate eighty , john cena , wwe...   \n",
       "9999  bureau labor statistic , donald trump , white ...   \n",
       "\n",
       "                     named_entities_article  \\\n",
       "0                                  abortion   \n",
       "1                                       NaN   \n",
       "2                                       NaN   \n",
       "3     Portland-Milwaukie Light Rail project   \n",
       "4                               Gary Farmer   \n",
       "...                                     ...   \n",
       "9995            Marilyn Manson,Wonder Years   \n",
       "9996                          George Carlin   \n",
       "9997                          Medical Codes   \n",
       "9998                              John Cena   \n",
       "9999                                    NaN   \n",
       "\n",
       "                                               keywords      source  \\\n",
       "0                                  abortion,health care  politifact   \n",
       "1                                              abortion  politifact   \n",
       "2                                                ethics  politifact   \n",
       "3            state budget,state finances,transportation  politifact   \n",
       "4                                                  guns  politifact   \n",
       "...                                                 ...         ...   \n",
       "9995  artists, asp article, music, radio & tv, telev...      snopes   \n",
       "9996                                        asp article      snopes   \n",
       "9997             health care,legal issues,public health  politifact   \n",
       "9998              death hoax, john cena, john cena dead      snopes   \n",
       "9999                                       economy,jobs  politifact   \n",
       "\n",
       "                      sourceURL  \\\n",
       "0     http://www.politifact.com   \n",
       "1     http://www.politifact.com   \n",
       "2     http://www.politifact.com   \n",
       "3     http://www.politifact.com   \n",
       "4     http://www.politifact.com   \n",
       "...                         ...   \n",
       "9995      http://www.snopes.com   \n",
       "9996      http://www.snopes.com   \n",
       "9997  http://www.politifact.com   \n",
       "9998      http://www.snopes.com   \n",
       "9999  http://www.politifact.com   \n",
       "\n",
       "                                                   link language  \n",
       "0     http://www.politifact.com/truth-o-meter/statem...  English  \n",
       "1     http://www.politifact.com/texas/statements/201...  English  \n",
       "2     http://www.politifact.com/texas/statements/201...  English  \n",
       "3     http://www.politifact.com/oregon/statements/20...  English  \n",
       "4     http://www.politifact.com/florida/statements/2...  English  \n",
       "...                                                 ...      ...  \n",
       "9995  https://www.snopes.com/fact-check/wondering-ab...  English  \n",
       "9996  https://www.snopes.com/fact-check/new-rules-fo...  English  \n",
       "9997  http://www.politifact.com/truth-o-meter/statem...  English  \n",
       "9998  https://www.snopes.com/fact-check/john-cena-de...  English  \n",
       "9999  http://www.politifact.com/missouri/statements/...  English  \n",
       "\n",
       "[14452 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#commencement des prétraitement :\n",
    "\n",
    "#import nécessaire :\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords') \n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import unicodedata\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "import inflect\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#La j'ai mis que la colonne text en lower mais je crois qu'on devra tous les mettre par la suite\n",
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "\n",
    "#White spaces removal\n",
    "df['text'] = df['text'].str.strip()\n",
    "\n",
    "#La j'ai mis que la colonne text en lower mais je crois qu'on devra tous les mettre par la suite\n",
    "df['keywords'] = df['keywords'].str.lower()\n",
    "\n",
    "\n",
    "#White spaces removal\n",
    "df['keywords'] = df['keywords'].str.strip()\n",
    "df['keywords'] = df['keywords'].fillna('nan')\n",
    "\n",
    "df['named_entities_claim']=df['named_entities_claim'].str.strip()\n",
    "df['named_entities_claim']=df['named_entities_claim'].str.lower()\n",
    "df['named_entities_claim'] = df['named_entities_claim'].fillna('nan')\n",
    "\n",
    "# df['headline']=df['headline'].str.strip()\n",
    "# df['headline']=df['headline'].str.lower()\n",
    "# df['headline'] = df['headline'].fillna('nan')\n",
    "\n",
    "\n",
    "#Elimination des stopWord :\n",
    "#input_str = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "#La liste des stopwords en Anglais\n",
    "negation ={'haven''t','cannot',\"doesn't\",\"shouldn't\",\"needn't\",\"shant't\",\"weren't\",\"hasn't\", \"wasn't\",\"didn't\", \"aren't\",'not', \"mightn't\", \"mustn't\", 'no',  \"wouldn't\", \"mightn't\", \"won't\",  \"needn't\", \"wasn't\", \"wouldn't\",  \"isn't\", \"doesn't\", \"weren't\", \"isn't\", \"hasn't\", \"hadn't\", \"don't\", \"hadn't\",\"couldn't\"}\n",
    "\n",
    "#La liste des stopwords de négation en Anglais  \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "a= df['text'].str.replace(\"’\",\"'\") #Extraire toutes les entrées de la colonne text\n",
    "pat = r'\\b(?:{})\\b'.format('|'.join(negation))\n",
    "\n",
    "a=a.str.replace(pat,'not')\n",
    "pat='\\w*\\d\\w*'\n",
    "text_without_stopwords=[] #Une liste dont on va affecter les textes après l'élimination des stop words\n",
    "tk=TweetTokenizer()\n",
    "for text in a.iteritems(): #On parcourt toutes les lignes\n",
    " tokens = tk.tokenize(str(text[1]))\n",
    " result = [i for i in tokens if not i in stop_words-negation]\n",
    " splitor=\" \"\n",
    " concatinated = splitor.join(result) #concatiner les tokens\n",
    " text_without_stopwords.append(concatinated)\n",
    "\n",
    "df['text']=text_without_stopwords\n",
    "\n",
    "a= df['named_entities_claim'].str.replace(\"’\",\"'\") #Extraire toutes les entrées de la colonne text\n",
    "text_without_stopwords=[] #Une liste dont on va affecter les textes après l'élimination des stop words\n",
    "tk=TweetTokenizer()\n",
    "for text in a.iteritems(): #On parcourt toutes les lignes\n",
    " tokens = tk.tokenize(str(text[1]))\n",
    " result = [i for i in tokens if not i in stop_words-negation]\n",
    " splitor=\" \"\n",
    " concatinated = splitor.join(result) #concatiner les tokens\n",
    " text_without_stopwords.append(concatinated)\n",
    "\n",
    "df['named_entities_claim']=text_without_stopwords\n",
    "\n",
    "\n",
    "#The following code removes this set of symbols [!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]:\n",
    "text_without_punctuation=[] # pour y mettre notre résultat\n",
    "a= df['text'] #Extraire toutes les entrées de la colonne text deja traiter pour faire la suite\n",
    "tk=TweetTokenizer()\n",
    "for text in a.iteritems(): #On parcourt toutes les lignes\n",
    "    result = re.sub('[!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]', '', str(text[1]) )\n",
    "    splitor=\"\" #séparateur de mots\n",
    "    concatinated = splitor.join(result) #concatiner les résultats\n",
    "    text_without_punctuation.append(concatinated)\n",
    "\n",
    "df['text']=text_without_punctuation\n",
    "\n",
    "\n",
    "#The following code removes non ASCII characters :\n",
    "text_without_ascii=[] # pour y mettre notre résultat\n",
    "a= df['text'] #Extraire toutes les entrées de la colonne text deja traiter pour faire la suite\n",
    "tk=TweetTokenizer()\n",
    "for text in a.iteritems(): #On parcourt toutes les lignes\n",
    "    text = unicodedata.normalize('NFKD', str(text[1]) ).encode(\"ascii\", \"ignore\").decode(\"utf-8\", 'ignore')\n",
    "    splitor=\"\" #séparateur de mots\n",
    "    concatinated = splitor.join(text)\n",
    "    text_without_ascii.append(concatinated)\n",
    "\n",
    "df['text']=text_without_ascii\n",
    "\n",
    "def nombreversmot(text):\n",
    "    if text.isdigit():\n",
    "        return p.number_to_words(text)\n",
    "    else: return text\n",
    "print(\"---------- Avant Transformation des numériques en mots -------------------\")\n",
    "#Transformation des numériques en mots\n",
    "numbertransf=[] # pour y mettre notre résultat\n",
    "p = inflect.engine()\n",
    "a= df['text'] #Extraire toutes les entrées de la colonne text deja traiter pour faire la suite\n",
    "tk=TweetTokenizer()\n",
    "for text in a.iteritems():\n",
    "     tokens = tk.tokenize(str(text[1]))\n",
    "     result = [nombreversmot(i) for i in tokens]\n",
    "     splitor=\" \"\n",
    "     concatinated = splitor.join(result) #concatiner les tokens\n",
    "     numbertransf.append(concatinated)\n",
    "\n",
    "df['text']=numbertransf\n",
    "df['text'] = df['text'].str.strip()\n",
    "\n",
    "numbertransf=[] # pour y mettre notre résultat\n",
    "p = inflect.engine()\n",
    "a= df['named_entities_claim'] #Extraire toutes les entrées de la colonne text deja traiter pour faire la suite\n",
    "tk=TweetTokenizer()\n",
    "for text in a.iteritems():\n",
    "     tokens = tk.tokenize(str(text[1]))\n",
    "     result = [nombreversmot(i) for i in tokens]\n",
    "     splitor=\" \"\n",
    "     concatinated = splitor.join(result) #concatiner les tokens\n",
    "     numbertransf.append(concatinated)\n",
    "\n",
    "df['named_entities_claim']=numbertransf\n",
    "df['named_entities_claim'] = df['named_entities_claim'].str.strip()\n",
    "print(df['named_entities_claim'].head(10))\n",
    "print(\"---------- Après Transformation des numériques en mots -------------------\")\n",
    "\n",
    "\n",
    "#Lemmatization :\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def nltk2wn_tag(nltk_tag):\n",
    "  if nltk_tag.startswith('J'):\n",
    "    return wordnet.ADJ\n",
    "  elif nltk_tag.startswith('V'):\n",
    "    return wordnet.VERB\n",
    "  elif nltk_tag.startswith('N'):\n",
    "    return wordnet.NOUN\n",
    "  elif nltk_tag.startswith('R'):\n",
    "    return wordnet.ADV\n",
    "  else:                    \n",
    "    return None\n",
    "def lemmatize_sentence(sentence):\n",
    "  nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))    \n",
    "  wn_tagged = map(lambda x: (x[0], nltk2wn_tag(x[1])), nltk_tagged)\n",
    "  res_words = []\n",
    "  for word, tag in wn_tagged:\n",
    "    if tag is None:                        \n",
    "      res_words.append(word)\n",
    "    else:\n",
    "      res_words.append(lemmatizer.lemmatize(word, tag))\n",
    "  return \" \".join(res_words)\n",
    "\n",
    "text_lemmatizer=[] # pour y mettre notre résultat\n",
    "a= df['text'] #Extraire toutes les entrées de la colonne text deja traiter pour faire la suite\n",
    "tk=TweetTokenizer()\n",
    "for ligne in a.iteritems(): #On parcourt toutes les lignes\n",
    "    newList=[]\n",
    "    newList.append(lemmatize_sentence(ligne[1])) #lemmatisation\n",
    "    splitor=\" \" #séparateur de mots\n",
    "    concatinated = splitor.join(newList)\n",
    "    text_lemmatizer.append(concatinated)\n",
    "df['text']=text_lemmatizer\n",
    "# #Lemmatization :\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# text_lemmatizer=[] # pour y mettre notre résultat\n",
    "# a= df['keywords'] #Extraire toutes les entrées de la colonne text deja traiter pour faire la suite\n",
    "# tk=TweetTokenizer()\n",
    "# for ligne in a.iteritems(): #On parcourt toutes les lignes \n",
    "#     wordList = re.sub(\"[^\\w]\", \" \",  ligne[1]).split() # on parcourt tout les mot de la ligne\n",
    "#     newList=[]\n",
    "#     for word in wordList:\n",
    "#         if not(word.isdigit()): # si c'est  mot et non un nombre\n",
    "#             newList.append(lemmatizer.lemmatize(word, pos = 'v')) #lemmatisation\n",
    "#     splitor=\" \" #séparateur de mots\n",
    "#     concatinated = splitor.join(newList)\n",
    "#     text_lemmatizer.append(concatinated)\n",
    "\n",
    "# df['keywords']=text_lemmatizer\n",
    "\n",
    "#Lemmatization :\n",
    "text_lemmatizer=[] # pour y mettre notre résultat\n",
    "a= df['named_entities_claim'] #Extraire toutes les entrées de la colonne text deja traiter pour faire la suite\n",
    "tk=TweetTokenizer()\n",
    "for ligne in a.iteritems(): #On parcourt toutes les lignes\n",
    "    newList=[]\n",
    "    newList.append(lemmatize_sentence(ligne[1])) #lemmatisation\n",
    "    splitor=\" \" #séparateur de mots\n",
    "    concatinated = splitor.join(newList)\n",
    "    text_lemmatizer.append(concatinated)\n",
    "df['named_entities_claim']=text_lemmatizer\n",
    "\n",
    "# #Lemmatization :\n",
    "# text_lemmatizer=[] # pour y mettre notre résultat\n",
    "# a= df['headline'] #Extraire toutes les entrées de la colonne text deja traiter pour faire la suite\n",
    "# tk=TweetTokenizer()\n",
    "# for ligne in a.iteritems(): #On parcourt toutes les lignes\n",
    "#     newList=[]\n",
    "#     newList.append(lemmatize_sentence(ligne[1])) #lemmatisation\n",
    "#     splitor=\" \" #séparateur de mots\n",
    "#     concatinated = splitor.join(newList)\n",
    "#     text_lemmatizer.append(concatinated)\n",
    "# df['headline']=text_lemmatizer\n",
    "\n",
    "\n",
    "\n",
    "# #suppression des common word:\n",
    "# word_counter  = Counter()\n",
    "# for sentence in df[\"text\"].values:\n",
    "#     for word in sentence.split():\n",
    "#         word_counter[word] += 1\n",
    "# most = word_counter.most_common(10)\n",
    "# print(\"most common word\"+str(most))\n",
    "# print(\"Suppression of the common word de nos artiles : \")\n",
    "# most_word = set([w for (w, wc) in most])\n",
    "# def delmost_word(sentence):\n",
    "#     return \" \".join([word for word in str(sentence).split() if word not in most_word])\n",
    "# df[\"text\"] = df[\"text\"].apply(delmost_word)\n",
    "# df[\"text\"].head()\n",
    "\n",
    "df\n",
    "\n",
    "#Fin des prétraitement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape 3 : Conversion des textes en valeurs numériques\n",
    " On transforme nos données de textes en valeurs numériques, en utilisant des LabelEncoder sur les colonnes qu'on a  sauf la colonne texte et la colonne keyword, dont on a essayé Tf-IDF pour les transformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       and  say  thousand  two\n",
      "0        0    0         0    0\n",
      "1        0    0         0    0\n",
      "2        0    1         0    0\n",
      "3        0    1         0    0\n",
      "4        0    1         0    0\n",
      "...    ...  ...       ...  ...\n",
      "14447    0    0         0    0\n",
      "14448    1    0         1    1\n",
      "14449    0    0         0    0\n",
      "14450    1    0         1    1\n",
      "14451    0    0         0    0\n",
      "\n",
      "[14452 rows x 4 columns]\n",
      "       barack  donald  facebook  house  http  internet  john  national  new  \\\n",
      "0           0       0         0      0     0         0     0         0    0   \n",
      "1           0       0         0      0     0         0     0         0    0   \n",
      "2           0       0         0      0     0         0     0         0    0   \n",
      "3           0       0         0      0     0         0     0         0    0   \n",
      "4           0       0         0      0     0         0     0         0    0   \n",
      "...       ...     ...       ...    ...   ...       ...   ...       ...  ...   \n",
      "14447       0       0         0      0     0         0     0         0    0   \n",
      "14448       0       0         0      0     0         0     0         0    1   \n",
      "14449       0       0         0      0     0         0     0         0    0   \n",
      "14450       0       0         0      0     0         0     1         0    0   \n",
      "14451       0       0         0      0     0         0     0         0    0   \n",
      "\n",
      "       news  ...  politifact  republican  state  time  trump  twitter  \\\n",
      "0         0  ...           0           0      0     0      0        0   \n",
      "1         0  ...           0           0      0     0      0        0   \n",
      "2         0  ...           0           0      0     0      0        0   \n",
      "3         0  ...           0           0      0     0      0        0   \n",
      "4         0  ...           0           0      0     0      0        0   \n",
      "...     ...  ...         ...         ...    ...   ...    ...      ...   \n",
      "14447     0  ...           0           0      0     0      0        0   \n",
      "14448     0  ...           0           0      0     0      0        0   \n",
      "14449     0  ...           0           0      0     0      0        0   \n",
      "14450     0  ...           0           0      0     0      0        0   \n",
      "14451     0  ...           0           0      2     0      0        0   \n",
      "\n",
      "       university  washington  white  york  \n",
      "0               0           0      0     0  \n",
      "1               0           0      0     0  \n",
      "2               0           0      0     0  \n",
      "3               0           0      0     0  \n",
      "4               0           0      0     0  \n",
      "...           ...         ...    ...   ...  \n",
      "14447           0           0      0     0  \n",
      "14448           0           0      0     0  \n",
      "14449           0           0      0     0  \n",
      "14450           0           0      0     0  \n",
      "14451           0           0      0     0  \n",
      "\n",
      "[14452 rows x 21 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/depinfo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:64: FutureWarning: The join_axes-keyword is deprecated. Use .reindex or .reindex_like on the result to achieve the same functionality.\n",
      "/home/depinfo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:65: FutureWarning: The join_axes-keyword is deprecated. Use .reindex or .reindex_like on the result to achieve the same functionality.\n",
      "/home/depinfo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:66: FutureWarning: The join_axes-keyword is deprecated. Use .reindex or .reindex_like on the result to achieve the same functionality.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#np.random.seed(500) #utilisé pour avoir le même résultat à chaque exécution \n",
    "#On crée des variable LabelEncoder qui vont servir à transférer nos données en valeurs numériques\n",
    "l1=LabelEncoder()\n",
    "l2=LabelEncoder()\n",
    "l3=LabelEncoder()\n",
    "l4=LabelEncoder()\n",
    "l5=LabelEncoder()\n",
    "l6=LabelEncoder()\n",
    "l7=LabelEncoder()\n",
    "l8=LabelEncoder()\n",
    "l9=LabelEncoder()\n",
    "l10=LabelEncoder()\n",
    "l11=LabelEncoder()\n",
    "l12=LabelEncoder()\n",
    "l13=LabelEncoder()\n",
    "df=df.applymap(str) #on transforme tous nous données en String car y'avais des entrées qui ont une combinaison du string et float\n",
    "#On applique la mesure TF-IDF sur la colonne text et la colonne keywords\n",
    "# Tfidf_vect = TfidfVectorizer(max_features=40000,min_df=0.03, max_df=0.7)\n",
    "# tfidfx=Tfidf_vect.fit_transform(df['named_entities_claim'])\n",
    "# df1 = pd.DataFrame(tfidfx.toarray(), columns=Tfidf_vect.get_feature_names())\n",
    "# Tfidf_vect = TfidfVectorizer(max_features=4000)\n",
    "# print(df['named_entities_claim'])\n",
    "cd = CountVectorizer(min_df=0.1)\n",
    "cd.fit(df[\"text\"])\n",
    "tfidfx=cd.transform(df['text'])\n",
    "df2 = pd.DataFrame(tfidfx.toarray(), columns=cd.get_feature_names())\n",
    "print(df2)\n",
    "cd = CountVectorizer(min_df=0.1)\n",
    "cd.fit(df[\"named_entities_claim\"])\n",
    "tfidfx=cd.transform(df['text'])\n",
    "df1 = pd.DataFrame(tfidfx.toarray(), columns=cd.get_feature_names())\n",
    "print(df1)\n",
    "\n",
    "# cd = CountVectorizer(min_df=0.1)\n",
    "# cd.fit(df[\"headline\"])\n",
    "# tfidfx=cd.transform(df['text'])\n",
    "# df3 = pd.DataFrame(tfidfx.toarray(), columns=cd.get_feature_names())\n",
    "# print(df3)\n",
    "\n",
    "#on transfère toutes les valeurs des colonnes qu'on va utiliser en valeurs numériques \n",
    "df['id']=l1.fit_transform(df['id'])\n",
    "df['text']=l3.fit_transform(df['text'])\n",
    "df['date']=l3.fit_transform(df['date'])\n",
    "df['author']=l5.fit_transform(df['author'])\n",
    "df['headline']=l6.fit_transform(df['headline'])\n",
    "# df['named_entities_claim']=l7.fit_transform(df['named_entities_claim'])\n",
    "df['named_entities_article']=l8.fit_transform(df['named_entities_article'])\n",
    "df['source']=l10.fit_transform(df['source'])\n",
    "df['sourceURL']=l11.fit_transform(df['sourceURL'])\n",
    "df['link']=l12.fit_transform(df['link'])\n",
    "df['language']=l13.fit_transform(df['language'])\n",
    "df['ratingName']=l13.fit_transform(df['ratingName'])\n",
    "\n",
    "\n",
    "df= pd.concat([df, df1], axis=1,join_axes=[df.index])\n",
    "df=pd.concat([df, df2], axis=1,join_axes=[df.index])\n",
    "df1=pd.concat([df1, df2], axis=1,join_axes=[df1.index])\n",
    "# df=pd.concat([df, df3], axis=1,join_axes=[df.index])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape 4 : Dévision du jeu de données en données d'entraînement et données de test\n",
    "On mélange le dataframe qu'on a et après on sélectionne 80% des données pour l'entraînement et 20% pour le test, ensuite on sélectionne les colonnes features dont on s'intéresse lors du classification, et on essaye plusieurs combinaisons de colonnes pour arriver à une meilleure accuracy (#TODO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3331027326184711\n",
      "Accuracy: 0.6841923209961951\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "##### \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "X_train, X_test ,y_train, y_test = train_test_split(df,df[\"truthRating\"], test_size=0.2, random_state=int(time.time()))\n",
    "\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "used_features =[\n",
    "#     \"author\",\n",
    "#     \"sourceURL\"\n",
    "    \n",
    "]\n",
    "for col in df1.columns :\n",
    "    used_features.append(col)\n",
    "for col in df2.columns :\n",
    "    used_features.append(col)\n",
    "# for col in df3.columns :\n",
    "#     used_features.append(col)\n",
    "\n",
    "gnb.fit(\n",
    "    X_train[used_features].values,\n",
    "    y_train\n",
    ")\n",
    "mnb=MultinomialNB()\n",
    "mnb.fit(X_train[used_features].values,\n",
    "    y_train)\n",
    "\n",
    "y_pred = gnb.predict(X_test[used_features])\n",
    "mnbpred=mnb.predict(X_test[used_features])\n",
    "# print(\"Number of mislabeled points out of a total {} points : {}, performance {:05.2f}%\"\n",
    "#       .format(\n",
    "#           X_test.shape[0],\n",
    "#           (X_test[\"ratingName\"] != y_pred).sum(),\n",
    "#           100*(1-(X_test[\"ratingName\"] != y_pred).sum()/X_test.shape[0])\n",
    "# ))\n",
    "# print(df['ratingName'])\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, mnbpred))\n",
    "print(len(used_features))##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape 5 :  Entraînement et prédiction en utilisant des modèles de \n",
    "\n",
    "\n",
    "classification\n",
    "On passe les features et targets des données d'entraînement à nos classifiers (#TODO tester plusieurs classifieurs) et après on lui laisse prédire les valeurs des données de test (soit vrai, faux ou mixture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin by importing all necessary libraries\n",
    "# import pandas as pd\n",
    "# import time\n",
    "# from sklearn.utils import shuffle\n",
    "# import numpy as np\n",
    "# ##### \n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "\n",
    "# #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=int(time.time()))\n",
    "# label = df[\"ratingName\"]\n",
    "# features=zip(df[\"date\"],df[\"author\"],df[\"named_entities_claim\"],df[\"source\"],df[\"sourceURL\"])\n",
    "# # dt = df[\"date\"]\n",
    "# # src = df[\"sourceURL\"]\n",
    "# # features=zip(dt,src)\n",
    "\n",
    "# pip\n",
    "\n",
    "# #print(df[\"author\"])  \n",
    "# print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
